{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09818f05",
   "metadata": {},
   "source": [
    "# Probability Distributions\n",
    "\n",
    "Data plays a central role in various fields, including Data Science and Machine Learning Engineering. As time progresses, data has become increasingly crucial for making informed decisions and developing innovative products. In this assignment, you will work with data that follows different probability distributions.\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. **Generating Data**: Learn how to generate data that follows specific probability distributions.\n",
    "\n",
    "2. **Naive Bayes Classifier (Continuous)**: Implement a Naive Bayes classifier for continuous data generated in Section 1.\n",
    "\n",
    "3. **Real-Life Problem (Spam Detection)**: Enhance the Naive Bayes implementation to address a real-life problem, specifically spam detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d0c4973",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import factorial\n",
    "from scipy.special import erfinv, comb\n",
    "from scipy.stats import uniform, binom, norm\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import utils\n",
    "from utils import (\n",
    "    estimate_gaussian_params,\n",
    "    estimate_binomial_params,\n",
    "    estimate_uniform_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8a7f2",
   "metadata": {},
   "source": [
    "# Section 1: Generating Data from a Specific Distribution\n",
    "\n",
    "Let's recap some concepts and formalize them to facilitate coding. Don't worry, you will be guided throughout the entire assignment!\n",
    "\n",
    "A random variable $X$ is a function that represents a random phenomenon, meaning its exact value cannot be determined. However, probabilities can be assigned to a set of possible values it can take. For example, if $X$ has a uniform distribution on $[2, 4]$, we cannot determine the exact value of $X$, but we can say with a probability of $1$ that the value lies between $[2, 4]$. We can also say that:\n",
    "\n",
    "$$\\mathbf P(X \\leq 3) = \\frac{1}{2}$$\n",
    "\n",
    "where $3$ is the midpoint of the interval $[2, 4]$. Therefore, you have learned that a random variable is associated with a function called the probability density function (PDF), which encodes the probability of the random variable falling within a given range. In other words, if $X$ is a continuous random variable and $f$ is its pdf, then:\n",
    "\n",
    "$$\\mathbf P(a \\leq X \\leq b) = \\text{Area of } f \\text{ between } a \\text{ and } b$$\n",
    "\n",
    "In the discrete case, $\\mathbf P(X = a) = f(a)$. In any case, $\\mathbf P(-\\infty < X < +\\infty) = 1$ because a random variable takes values on real numbers, and you're covering all possible outcomes.\n",
    "\n",
    "Another function associated with a random variable is the cumulative distribution function (CDF), denoted as $F$. It represents the probability that a random variable $X$ will be less than or equal to $x$, for any $x \\in \\mathbb{R}$:\n",
    "\n",
    "$$F(x) := \\mathbf P(X \\leq x), \\quad \\forall x \\in \\mathbb{R}$$\n",
    "\n",
    "The CDF is a non-decreasing function and approaches $1$ as $x$ approaches infinity because it represents a probability and must yield a value between $0$ and $1$.\n",
    "\n",
    "In the lectures, you learned that if $X$ is a random variable with cdf $F$, then $F(X)$ follows a uniform distribution between $0$ and $1$. In other words, the new random variable $F(X)$ will be uniformly distributed between $0$ and $1$. This opens up the possibility of generating artificial data with any desired distribution, given that we know $F$. The process is as follows:\n",
    "\n",
    "1. Generate a random value $y$ uniformly from the interval $[0, 1]$.\n",
    "2. Compute $F^{-1}(y)$, which is the inverse function of $F$ evaluated at $y$.\n",
    "\n",
    "It can be shown that if $Y$ follows a uniform distribution between $0$ and $1$, then the random variable $F^{-1}(Y)$ has the same distribution as $X$.\n",
    "\n",
    "Therefore, by computing the inverse of $F$, you can generate artificial data from any known distribution! This is an incredibly powerful technique, isn't it?\n",
    "\n",
    "So far in the course, you have encountered three common probability distributions:\n",
    "- Uniform\n",
    "- Binomial\n",
    "- Gaussian\n",
    "\n",
    "In the first part of this assignment, you will code a random generator for each of the above distributions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b9b4a",
   "metadata": {},
   "source": [
    "## Exercise 1: Uniform Generator\n",
    "\n",
    "The natural first step is to create a function capable of generating random data that comes from the uniform distribution. You will not be coding a [pseudo-random number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) (this is outside the scope of this assignment) but instead you will use a predefined function that handles this for you. If you are unsure where you can find such a function take a look at the [numpy.random.uniform](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b2dcd2",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def uniform_generator(a, b, num_samples=100):\n",
    "    \"\"\"\n",
    "    Generates an array of uniformly distributed random numbers within the specified range.\n",
    "\n",
    "    Parameters:\n",
    "    - a (float): The lower bound of the range.\n",
    "    - b (float): The upper bound of the range.\n",
    "    - num_samples (int): The number of samples to generate (default: 100).\n",
    "\n",
    "    Returns:\n",
    "    - array (ndarray): An array of random numbers sampled uniformly from the range [a, b).\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "\n",
    "    array = np.random.uniform(a, b, num_samples)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02cb0af5",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 randomly generated values between 0 and 1:\n",
      "[0.375 0.951 0.732 0.599 0.156 0.156]\n",
      "\n",
      "3 randomly generated values between 20 and 55:\n",
      "[33.109 53.275 45.62 ]\n",
      "\n",
      "1 randomly generated value between 0 and 100:\n",
      "[37.454]\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "print(f\"6 randomly generated values between 0 and 1:\\n{np.array2string(uniform_generator(0, 1, num_samples=6), precision=3)}\\n\")\n",
    "print(f\"3 randomly generated values between 20 and 55:\\n{np.array2string(uniform_generator(20, 55, num_samples=3), precision=3)}\\n\")\n",
    "print(f\"1 randomly generated value between 0 and 100:\\n{np.array2string(uniform_generator(0, 100, num_samples=1), precision=3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871446b7",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "6 randomly generated values between 0 and 1:\n",
    "[0.375 0.951 0.732 0.599 0.156 0.156]\n",
    "\n",
    "3 randomly generated values between 20 and 55:\n",
    "[33.109 53.275 45.62 ]\n",
    "\n",
    "1 randomly generated value between 0 and 100:\n",
    "[37.454]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86f5e76",
   "metadata": {},
   "source": [
    "## Exercise 2:  Gaussian Generator\n",
    "\n",
    "With your uniform data generator ready you can go ahead and create generators for the other distributions. In order to do this you will need the inverse `CDF` for the distribution you wish to create data for. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d63b3b",
   "metadata": {},
   "source": [
    "### 2.1 Inverse CDF - Gaussian\n",
    "\n",
    "Let's start with a Normal Distribution generator. In the case of the Gaussian distribution finding the inverse is a bit challenging because the CDF doesn't have a closed analytical expression.\n",
    "\n",
    "The closed formula uses a function called the Gaussian error function, denoted as $\\text{erf}(x)$. However, you don't need to implement it or its inverse for this assignment. These functions are important in statistics, and there are many libraries available that provide their implementations.\n",
    "\n",
    "For instance, you can use [scipy.special.erf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.erf.html) and [scipy.special.erfinv](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.erfinv.html#scipy.special.erfinv) to compute the erf function and its inverse. Alternatively, you can find an implementation of the erf function in [math.erf](https://docs.python.org/3/library/math.html#math.erf) from the Python math library.\n",
    "\n",
    "If $X \\sim N(\\mu, \\sigma)$, then the CDF $F(x)$ can be expressed as:\n",
    "\n",
    "$$y = F(x) = \\frac{1}{2} \\left[ 1 + \\text{erf}\\left( \\frac{x - \\mu}{\\sigma \\sqrt{2}} \\right) \\right].$$\n",
    "\n",
    "With some simple calculations and denoting ${\\text{erf}}^{-1}$ as the inverse of the $\\text{erf}$ function, it can be shown that:\n",
    "\n",
    "$$x = F^{-1}(y) = \\sigma \\sqrt{2} \\cdot \\text{erf}^{-1}(2y - 1) + \\mu.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20aaede",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def inverse_cdf_gaussian(y, mu, sigma):\n",
    "    \"\"\"\n",
    "    Calculates the inverse cumulative distribution function (CDF) of a Gaussian distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - y (float or ndarray): The probability or array of probabilities.\n",
    "    - mu (float): The mean of the Gaussian distribution.\n",
    "    - sigma (float): The standard deviation of the Gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "    - x (float or ndarray): The corresponding value(s) from the Gaussian distribution that correspond to the given probability/ies.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "#   x = sigma * np.sqrt(2) * erfinv(2 * y - 1) + mu # @OMIT\n",
    "    \n",
    "    x = norm.ppf(y, mu, sigma)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc12aaba",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse of Gaussian CDF with mu 15 and sigma 5 for value 1e-10: -16.807\n",
      "Inverse of Gaussian CDF with mu 15 and sigma 5 for value 0: -inf\n",
      "Inverse of Gaussian CDF with mu 20 and sigma 0.5 for value 0.4: 19.873\n",
      "Inverse of Gaussian CDF with mu 15 and sigma 5 for value 1: inf\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "print(f\"Inverse of Gaussian CDF with mu {15} and sigma {5} for value {1e-10}: {inverse_cdf_gaussian(1e-10, 15, 5):.3f}\")\n",
    "print(f\"Inverse of Gaussian CDF with mu {15} and sigma {5} for value {0}: {inverse_cdf_gaussian(0, 15, 5)}\")\n",
    "print(f\"Inverse of Gaussian CDF with mu {20} and sigma {0.5} for value {0.4}: {inverse_cdf_gaussian(0.4, 20, 0.5):.3f}\")\n",
    "print(f\"Inverse of Gaussian CDF with mu {15} and sigma {5} for value {1}: {inverse_cdf_gaussian(1, 15, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a401b18",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Inverse of Gaussian CDF with mu 15 and sigma 5 for value 1e-10: -16.807\n",
    "Inverse of Gaussian CDF with mu 15 and sigma 5 for value 0: -inf\n",
    "Inverse of Gaussian CDF with mu 20 and sigma 0.5 for value 0.4: 19.873\n",
    "Inverse of Gaussian CDF with mu 15 and sigma 5 for value 1: inf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fbc248",
   "metadata": {},
   "source": [
    "### 2.2 Gaussian Generator\n",
    "\n",
    "Now that you have all the necessary information, you can create a generator for data that follows a Gaussian distribution with a specified $\\mu$ and $\\sigma$. Similar to the generator for uniformly distributed data, the `gaussian_generator` function should allow you to specify the number of samples to generate. **Make sure to utilize the functions you have defined earlier in the assignment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df743c1",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def gaussian_generator(mu, sigma, num_samples):\n",
    "\n",
    "    # Generate an array with num_samples elements that distribute uniformally between 0 and 1\n",
    "    u = uniform_generator(0, 1, num_samples)\n",
    "\n",
    "    # Use the uniform-distributed sample to generate Gaussian-distributed data\n",
    "    # Hint: You need to sample from the inverse of the CDF of the distribution you are generating\n",
    "    array = inverse_cdf_gaussian(u, mu, sigma)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07885432",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAEWCAYAAAC3wpkaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAt7klEQVR4nO3de3xV1Zn/8c9TQKIEhQJFBC3ghaKJpJBAHTCDVSeMrWCBHyOtHcLUxA7F1rYzRTszijCttjrFdiy1ONaRKWjbUJXieBcGxmnl1qQJ0KpFqERUilJACBB5fn/sHXoIuZwkZ++Tc/i+X6+8cvZt7Wevs4GHtdZe29wdEREREYneB9IdgIiIiMjJQomXiIiISEyUeImIiIjERImXiIiISEyUeImIiIjERImXiIiISEyUeInEwMw2mdn4dMfRGZlZfzNbbWb7zOzf0h1PMszs62b2HzGda5uZXRHFec1sv5kNDT//p5n9awrLvs/M/iVV5Ylki67pDkAk05nZNuB6d38uYV1puG4cgLtflEQ5g4HXgG7uXh9JsJ1TOfBH4HRvZmJBMysE5gJjAQPeAB4F7nb3d2OK8xh3/2bc52zLec1sFfBjd28xSXP33FTE1fh+D8v+fCrKFsk2avESOUmYWWf9j9aHgc0tJF1/AawCXgQ+4u69gAlAPTAiphizSie+F0SynhIvkRg06i4abWbrzWyvmb1lZt8Jd1sd/t4TdgFdYmYfMLN/NrPtZva2mS02szMSyv3bcNtuM/uXRueZa2YVZvZjM9sLlIbn/qWZ7TGznWZ2r5mdklCem9ksM3sl7Pqbb2bnmtn/hfH+tGF/M+trZivCst4xszVm1uTfKWb2F2a2zsz+FP7+i3D9fwIzgK+F13xFE4d/G3jQ3e9w97cA3P0P7n6bu68KyznXzF4I6+GPZrbEzHo1uq7zEpaPdau1dB1mNsfMasO6+J2ZXZ5Qtz9OKO9nZvZmeH2rzeyiRuf6vpk9EZbzkpmd28K98tmE7/SfGm07dl4zywm/291h7Oss6Lb9BnApcG9Yp/cm1MEXzOwV4JWm6gXoa2bPhnH+j5l9ONxvcLhv14RYVpnZ9WY2HLgPuCQ8357GdRwul5nZq2EdLzezsxp9P58P77s9YX1ZuO28MJY/hd/tT5qrO5FMoMRLJH7fBb7r7qcD5wI/DdcXh797uXuuu/8SKA1/LgOGArlAwz+kFwILgc8AA4AzgIGNzjUJqAB6AUuA94EvA32BS4DLgVmNjikBRgEfA74GLAKuA84G8oDp4X5fBXYA/YD+wNeBE1qtzOyDwBPA94A+wHeAJ8ysj7uXhnF9O7zm5xod2yOMc1njchufBrgDOAsYHsY6t5VjGjR5HWY2DJgNFLl7T4J62dZMGU8C5wMfAjaG15ToWuB2oDfwKvCNJi8i+E5/AHw2vJY+wKBmzjmD4Ds/O9zv88BBd/8nYA0wO6zT2QnHXAOMAS5spszPAPMJ7o/KJq7jBO6+JTz3L8Pz9Wriuj5O8P1MI7hXtwOPNNrtk0ARcHG4X0m4fj7wDEHdDQL+vbWYRDozJV4iqfFY+D/1PeH/+Be2sO8R4Dwz6+vu+939Vy3s+xngO+6+1d33A7cA14YtD1OBX7j7/7r7YeBWTkx8funuj7n7UXc/6O4b3P1X7l7v7tuAHwJ/2eiYb7v7XnffBNQAz4Tn/xNBgvHRhOsYAHzY3Y+4+5pmugs/Abzi7v8Vnvdh4LfA1S1cd4PeBH9Pvdmwwsy+Hdbze2b2zwDu/qq7P+vuh9x9F0Fy1/i6mtPcdbwPdAcuNLNu7r7N3X/fVAHu/iN33+fuhwgSvhGW0DIJPOrua8Oxe0uAgmZimQqscPfVYVn/AhxtIe4+wHnu/n743e5t5VrvcPd33P1gM9ufSDj3PxG0Yp3dSpnJ+AzwI3ffGJZ9S1j24IR97nT3Pe7+B2Alf66jIwTd0We5e527/28K4hFJGyVeIqlxjbv3avjhxFakRJ8DLgB+G3YPfbKFfc8iaB1osJ3goZj+4bbXGza4+wFgd6PjX09cMLMLwm61Ny3ofvwmQetGorcSPh9sYrlhQPZdBK03z5jZVjO7OclraLiOxq1zTXmXIPEY0LDC3b8W1vGjhA8IhV1sj4TdgnuBHzdxXc1p8jrc/VXgJoJE6u2w/LMaH2xmXczsTjP7fXjubeGmxPO/mfD5AH+uw8Yaf6fvceJ32uC/gKeBR8zsjTAh7dbilTa6H1raHib674QxddRx90BY9m6Ovweaq6OvEbRorrXg6eC/S0E8ImmjxEskZu7+irtPJ+iW+hZQEXapNdVa9AbB//YbnEMwqPwtYCcJ3VBmdipBC8hxp2u0/AOC1qbzw67OrxP8o9ae69jn7l9196HAROArDWOgWrmGhuuoTeIc7wEvAZNb2fWbBNeaH17XdRx/XQeA0xKWz0zmOtx9afik3ofD8r/VxLk/TdClewVB19/gcH176nUnQddhUIDZaZz4nTbEfcTdb3f3C4G/IOiq+9uGzc2U39z6BonnzgU+SPD9vReubrIOkyj3uHsgvN/7kNw98Ka7l7n7WcANwMJG49JEMooSL5GYmdl1ZtbP3Y8Ce8LVR4Fd4e+hCbs/DHzZzIaE/xB+E/hJ2GVVAVxtwcD1UwhaZlr7x74nsBfYb2YfAf6+A9fxyXDgswF/Iuiaa6pb7L+BC8zs02bW1cz+hmCM0YokT/U14O/M7GYz+1B47kHAkIR9egL7gT+Z2UDgHxuVUQl8OmydmkBCN2Rz12Fmw8zs42bWHagjaO1r6vp6AocIWnBOI/iO2qsC+KSZjQu/03k08/e0mV1mZvlm1oXgOz2SEN9bHH8fJeuqhHPPB37l7q+H3be1wHVhHf4dwfjEBm8BgyzhQY1GHgZmmllBWJ/fBF4Ku7tbZGb/L/y+IWgBdZrvfhXp9JR4icRvArDJzPYTDLS/Nhx/dYBg0PWL4RimjwE/IuhSWk0wx1cdcCNAOAbrRoJByjsJEo+3CZKA5vwDQQvNPuB+oCNPiJ0PPBee95fAQndf2Xgnd99N0BrzVYLk5GvAJ939j8mcJBzT83GChw9eDsfQPUUwxUTDQOvbgZEEidMTwM8bFfMlgjFlewjGGz2WxHV0B+4kmGPsTYIWyluaCHExQTdaLbAZaGnMXovC7/QLwFKC7/RdgoH/TTmTIFHbC2wB/ofgXoHgvppqZu+a2ffaEMJS4DaCLsZRBC2HDcoIEtrdwEXA/yVsewHYBLxpZid8r+FDE/9C8JDEToKk7dokYyoCXgr/vCwHvuTuW9twTSKdijUzdY6IZJiwRWwPQTfia2kOR0REmqAWL5EMZmZXm9lp4ZiZu4Fqmp/yQERE0kyJl0hmm0QwcPkNgi6za5ubAV5ERNJPXY0iIiIiMVGLl4iIiEhMMuJFqX379vXBgwenOwwRERGRVm3YsOGP7t6vqW0ZkXgNHjyY9evXpzsMERERkVaZWeO3dRyjrkYRERGRmCjxEhEREYmJEi8RERGRmGTEGC8RERFJzpEjR9ixYwd1dXXpDiXr5eTkMGjQILp165b0MUq8REREssiOHTvo2bMngwcPJnj3u0TB3dm9ezc7duxgyJAhSR+nrkYREZEsUldXR58+fZR0RczM6NOnT5tbFpV4iYiIZBklXfFoTz0r8RIRERGJicZ4iYiIZLEFz76c0vK+fOUFKS3vZKPEKws1/kOmPyQpsvKO1ve57Jbo48giCysXduj4WQWzUhSJiGSyW2+9leLiYq644oqUlXnHHXfwwAMP0KVLF773ve9RUlKSknKVeImIiEhGmzdvXkrL27x5M4888gibNm3ijTfe4IorruDll1+mS5cuHS5bY7xEREQkpebPn8+wYcMYN24c06dP5+677+b++++nqKiIESNGMGXKFA4cOABAaWkpFRUVx47Nzc0FYOfOnRQXF1NQUEBeXh5r1qzh/fffp7S0lLy8PPLz81mwYMEJZcybN4+ioiLy8vIoLy/H3QEYP348c+bMYfTo0VxwwQWsWbOm2fgff/xxrr32Wrp3786QIUM477zzWLt2bUrqRomXiIiIpMy6detYtmwZVVVVPPnkk6xfvx6AyZMns27dOqqqqhg+fDgPPPBAi+UsXbqUkpISKisrqaqqoqCggMrKSmpra6mpqaG6upqZM2eecNzs2bNZt24dNTU1HDx4kBUrVhzbVl9fz9q1a7nnnnu4/fbbmz13bW0tZ5999rHlQYMGUVtb29aqaFKkiZeZfdnMNplZjZk9bGY5ZjbEzF4ys1fN7CdmdkqUMYiIiEh8XnzxRSZNmkROTg49e/bk6quvBqCmpoZLL72U/Px8lixZwqZNm1osp6ioiAcffJC5c+dSXV1Nz549GTp0KFu3buXGG2/kqaee4vTTTz/huJUrVzJmzBjy8/N54YUXjjvP5MmTARg1ahTbtm1L3UW3QWSJl5kNBL4IFLp7HtAFuBb4FrDA3c8D3gU+F1UMIiIi0jmUlpZy7733Ul1dzW233XZs4tGuXbty9OhRAI4ePcrhw4cBKC4uZvXq1QwcOJDS0lIWL15M7969qaqqYvz48dx3331cf/31x52jrq6OWbNmUVFRQXV1NWVlZcdNcNq9e3cAunTpQn19fbOxDhw4kNdff/3Y8o4dOxg4cGBK6iHqwfVdgVPN7AhwGrAT+Djw6XD7Q8Bc4AcRxyEiInJSivvJ9rFjx3LDDTdwyy23UF9fz4oVKygvL2ffvn0MGDCAI0eOsGTJkmOJzODBg9mwYQPTpk1j+fLlHDlyBIDt27czaNAgysrKOHToEBs3buSqq67ilFNOYcqUKQwbNozrrrvuuHM3JFl9+/Zl//79VFRUMHXq1DZfw8SJE/n0pz/NV77yFd544w1eeeUVRo8e3cGaCUSWeLl7rZndDfwBOAg8A2wA9rh7Q5q5A2gyhTSzcqAc4JxzzokqTBEREUmhoqIiJk6cyMUXX0z//v3Jz8/njDPOYP78+YwZM4Z+/foxZswY9u3bB0BZWRmTJk1ixIgRTJgwgR49egCwatUq7rrrLrp160Zubi6LFy+mtraWmTNnHmshu+OO46f56dWrF2VlZeTl5XHmmWdSVFTUrmu46KKLmDZtGhdeeCFdu3bl+9//fkqeaASwhtH+qWZmvYFlwN8Ae4CfARXA3LCbETM7G3gy7IpsVmFhoTcMzpPWaR6viGger5TTPF4iqbdlyxaGDx+e1hj2799Pbm4uBw4coLi4mEWLFjFy5Mi0xhSVpurbzDa4e2FT+0fZ1XgF8Jq77wqD+DkwFuhlZl3DVq9BQGoeExAREZFOoby8nM2bN1NXV8eMGTOyNulqjygTrz8AHzOz0wi6Gi8H1gMrganAI8AM4PEIYxCObwFT61fEkmkVA7WMpYhazEQ6p6VLl6Y7hKQ8/fTTzJkz57h1Q4YM4dFHH43snFGO8XrJzCqAjUA98GtgEfAE8IiZ/Wu4ruWJPEREREQiUFJSkrJXASUr0qca3f024LZGq7cCqXk0QERERCSDaOZ6ERERkZgo8RIRERGJSdQTqIqIiEg6JfvQT7L0cFCHqMVLREREMtqtt97Kc889l7Lydu/ezWWXXUZubi6zZ89OWbmgFi8RERHJcPPmzUtpeTk5OcyfP5+amhpqampSWrZavERERCSl5s+fz7Bhwxg3bhzTp0/n7rvv5v7776eoqIgRI0YwZcoUDhw4AAQvz66oqDh2bG5uLgA7d+6kuLiYgoIC8vLyWLNmDe+//z6lpaXk5eWRn5/PggULTihj3rx5FBUVkZeXR3l5OQ1v6Bk/fjxz5sxh9OjRXHDBBaxZs6bZ+Hv06MG4cePIyclJed0o8RIREZGUWbduHcuWLaOqqoonn3yShlf+TZ48mXXr1lFVVcXw4cN54IGWp/FcunQpJSUlVFZWUlVVRUFBAZWVldTW1lJTU0N1dTUzZ8484bjZs2ezbt06ampqOHjwICtWrDi2rb6+nrVr13LPPfdw++23p/bCk6TES0RERFLmxRdfZNKkSeTk5NCzZ0+uvvpqAGpqarj00kvJz89nyZIlbNq0qcVyioqKePDBB5k7dy7V1dX07NmToUOHsnXrVm688UaeeuopTj/99BOOW7lyJWPGjCE/P58XXnjhuPNMnjwZgFGjRrFt27bUXXQbKPESERGRyJWWlnLvvfdSXV3NbbfdRl1dHQBdu3bl6NGjABw9epTDhw8DUFxczOrVqxk4cCClpaUsXryY3r17U1VVxfjx47nvvvu4/vrrjztHXV0ds2bNoqKigurqasrKyo6dB6B79+4AdOnShfr6+jgu+wQaXC8iIpLNYp7+YezYsdxwww3ccsst1NfXs2LFCsrLy9m3bx8DBgzgyJEjLFmyhIEDBwIwePBgNmzYwLRp01i+fDlHjhwBYPv27QwaNIiysjIOHTrExo0bueqqqzjllFOYMmUKw4YN47rrrjvu3A1JVt++fdm/fz8VFRVMnTo11utvjRIvERERSZmioiImTpzIxRdfTP/+/cnPz+eMM85g/vz5jBkzhn79+jFmzBj27dsHQFlZGZMmTWLEiBFMmDCBHj16ALBq1SruuusuunXrRm5uLosXL6a2tpaZM2ceayG7447j5yjr1asXZWVl5OXlceaZZ1JUVNTu6xg8eDB79+7l8OHDPPbYYzzzzDNceOGF7S6vgTWM9u/MCgsLvWFwnrRuwbMvN7vty1deEGMkWSaVkxBqAkIAFlYuTOv5ZxXMSuv5RaKwZcsWhg8fntYY9u/fT25uLgcOHKC4uJhFixYxcuTItMYUlabq28w2uHthU/urxUtERERSqry8nM2bN1NXV8eMGTOyNulqDyVeIiIiklJLly5NdwhJefrpp5kzZ85x64YMGcKjjz4a2TmVeGWwxC5FdSE2I5nuwZO426+jXX3qqhORTFZSUkJJSUms59R0EiIiIiIxiSzxMrNhZlaZ8LPXzG4ysw+a2bNm9kr4u3dUMYiIiIh0JpElXu7+O3cvcPcCYBRwAHgUuBl43t3PB54Pl0VERESyXlxjvC4Hfu/u281sEjA+XP8QsAqY08xxIiIi0gGpnrZFYzs7Jq4xXtcCD4ef+7v7zvDzm0D/pg4ws3IzW29m63ft2hVHjCIiIpKBbr31Vp577rmUlffss88yatQo8vPzGTVqFC+88ELKyo68xcvMTgEmAic8OububmZNzuDq7ouARRBMoBppkCIiIpKx5s2bl9Ly+vbtyy9+8QvOOussampqKCkpoba2NiVlx9Hi9dfARnd/K1x+y8wGAIS/344hBhEREYnJ/PnzGTZsGOPGjWP69Oncfffd3H///RQVFTFixAimTJnCgQMHgODl2RUVFceOzc3NBWDnzp0UFxdTUFBAXl4ea9as4f3336e0tJS8vDzy8/NZsGDBCWXMmzePoqIi8vLyKC8vp+ENPePHj2fOnDmMHj2aCy64gDVr1jQb/0c/+lHOOussAC666CIOHjzIoUOHUlI3cSRe0/lzNyPAcmBG+HkG8HgMMYiIiEgM1q1bx7Jly6iqquLJJ5+k4ZV/kydPZt26dVRVVTF8+HAeeOCBFstZunQpJSUlVFZWUlVVRUFBAZWVldTW1lJTU0N1dTUzZ8484bjZs2ezbt06ampqOHjwICtWrDi2rb6+nrVr13LPPfdw++23J3U9y5YtY+TIkXTv3r0NtdC8SBMvM+sBXAn8PGH1ncCVZvYKcEW4LCIiIlngxRdfZNKkSeTk5NCzZ0+uvvpqAGpqarj00kvJz89nyZIlbNq0qcVyioqKePDBB5k7dy7V1dX07NmToUOHsnXrVm688UaeeuopTj/99BOOW7lyJWPGjCE/P58XXnjhuPNMnjwZgFGjRrFt27ZWr2XTpk3MmTOHH/7wh22ogZZFmni5+3vu3sfd/5Swbre7X+7u57v7Fe7+TpQxiIiISPqVlpZy7733Ul1dzW233UZdXR0AXbt25ejRowAcPXqUw4cPA1BcXMzq1asZOHAgpaWlLF68mN69e1NVVcX48eO57777uP766487R11dHbNmzaKiooLq6mrKysqOnQc41mrVpUsX6uvrW4x3x44dfOpTn2Lx4sWce+65KasHvTJIREQki8U9/cPYsWO54YYbuOWWW6ivr2fFihWUl5ezb98+BgwYwJEjR1iyZAkDBw4EYPDgwWzYsIFp06axfPlyjhw5AsD27dsZNGgQZWVlHDp0iI0bN3LVVVdxyimnMGXKFIYNG8Z111133Lkbkqy+ffuyf/9+KioqmDp1apuvYc+ePXziE5/gzjvvZOzYsR2skeMp8RIREZGUKSoqYuLEiVx88cX079+f/Px8zjjjDObPn8+YMWPo168fY8aMYd++fQCUlZUxadIkRowYwYQJE+jRowcAq1at4q677qJbt27k5uayePFiamtrmTlz5rEWsjvuOP59vL169aKsrIy8vDzOPPNMioqK2nUN9957L6+++irz5s079sTkM888w4c+9KH2Vssx1jDavzMrLCz0hsF5J7vEF2MnSnxJdnP7NN4voyXz8utkJfuS7HScMwmpnhyxLTr6P+l0xg6aCFKy05YtWxg+fHhaY9i/fz+5ubkcOHCA4uJiFi1axMiRI9MaU1Saqm8z2+DuhU3trxYvERERSany8nI2b95MXV0dM2bMyNqkqz2UeImIiEhKLV26NN0hJOXpp59mzpzj31o4ZMgQHn300cjOqcRLREQky7g7ZpbuMDq9kpISSkpK2n18e4ZrxfWuRhEREYlBTk4Ou3fvbldSIMlzd3bv3k1OTk6bjlOLVwZoabB8VkrlIPYMkO4B5iKSXQYNGsSOHTvYtWtXukPJejk5OQwaNKhNxyjxEhERySLdunVjyJAh6Q5DmqGuRhEREZGYKPESERERiYkSLxEREZGYaIxXljjpBuCn0kk2mD+V9GCAiEjbqMVLREREJCZKvERERERiosRLREREJCaRJl5m1svMKszst2a2xcwuMbMPmtmzZvZK+Lt3lDGIiIiIdBZRt3h9F3jK3T8CjAC2ADcDz7v7+cDz4bKIiIhI1oss8TKzM4Bi4AEAdz/s7nuAScBD4W4PAddEFYOIiIhIZxJli9cQYBfwoJn92sz+w8x6AP3dfWe4z5tA/6YONrNyM1tvZuv1vikRERHJBlEmXl2BkcAP3P2jwHs06lb04NXpTb4+3d0XuXuhuxf269cvwjBFRERE4hFl4rUD2OHuL4XLFQSJ2FtmNgAg/P12hDGIiIiIdBqRzVzv7m+a2etmNszdfwdcDmwOf2YAd4a/H48qhkzSeOb5L195QZoiaadkZn+/7Jbo42iHhXt+06HjZ/W6OEWRiIhItov6lUE3AkvM7BRgKzCToJXtp2b2OWA7MC3iGEREREQ6hUgTL3evBAqb2HR5lOcVERER6Yw0c72IiIhITJR4iYiIiMREiZeIiIhITJR4iYiIiMQk6qcapZNJnLYi46asyCaJ0280N53FkEvjieUktrByYYeOn1UwK0WRiMjJQi1eIiIiIjFR4iUiIiISE3U1CqAuyIz22prW91G3ZafUka5OdXOKZKakWrzM7Fwz6x5+Hm9mXzSzXpFGJiIiIpJlku1qXAa8b2bnAYuAs4GlkUUlIiIikoWSTbyOuns98Cng3939H4EB0YUlIiIikn2STbyOmNl0YAawIlzXLZqQRERERLJTsonXTOAS4Bvu/pqZDQH+K7qwRERERLJPUk81uvtmM5sDnBMuvwZ8K8rARERERLJNsk81Xg1UAk+FywVmtjzCuERERESyTrJdjXOB0cAeAHevBIZGEpGIiIhIlkp2AtUj7v4nM0tcd7S1g8xsG7APeB+od/dCM/sg8BNgMLANmObu77YhZhEREZGMlGyL1yYz+zTQxczON7N/B/4vyWMvc/cCdy8Ml28Gnnf384Hnw2URERGRrJds4nUjcBFwCHgY2Avc1M5zTgIeCj8/BFzTznJEREREMkqyTzUeAP4p/GkLB54xMwd+6O6LgP7uvjPc/ibQv6kDzawcKAc455xz2nhaERERkc6nxcTLzO5x95vM7BcESdRx3H1iK+WPc/daM/sQ8KyZ/bbR8R4mZScIk7RFAIWFhU3uIyIiIpJJWmvxapgk9e72FO7uteHvt83sUYInI98yswHuvtPMBgBvt6dsERERkUzTYuLl7hvCj+uBg+5+FMDMugDdWzrWzHoAH3D3feHnvwLmAcsJXj10Z/j78Q5dgYiIiEiGSHZw/fPAaQnLpwLPtXJMf+B/zawKWAs84e5PESRcV5rZK8AV4bKIiIhI1kt2Hq8cd9/fsODu+83stJYOcPetwIgm1u8GLm9TlCIiIiJZINkWr/fMbGTDgpmNAg5GE5KIiIhIdkq2xesm4Gdm9gZgwJnA30QVlIiIiEg2SnYer3Vm9hFgWLjqd+5+JLqwJA4Lnn053SFIS15bk+4IpBULKxemOwQRyTDJtngBFBG8X7ErMNLMcPfFkUQlIiIikoWSSrzM7L+Ac4FKghdeQzChqhIvERERkSQl2+JVCFzo7ppBXkRERKSdkn2qsYZgQL2IiIiItFOyLV59gc1mthY41LAyiXc1ioiIiEgo2cRrbpRBiIiIiJwMkp1O4n/M7MPA+e7+XDhrfZdoQ8t+GTGdw8o70h1Bqxbu+U26QxAREUlKUmO8zKwMqAB+GK4aCDwWUUwiIiIiWSnZwfVfAMYCewHc/RXgQ1EFJSIiIpKNkh3jdcjdD5sZAGbWlWAeL4lIRnRDtlUGdFu2h7o6RUQkWcm2eP2PmX0dONXMrgR+BvwiurBEREREsk+yidfNwC6gGrgB+G/gn6MKSkRERCQbJftU41Hg/vBHRERERNoh2Xc1vkYTY7rcfWgSx3YB1gO17v5JMxsCPAL0ATYAn3X3w22KWkRERCQDJdvVWAgUhT+XAt8DfpzksV8CtiQsfwtY4O7nAe8Cn0uyHBEREZGMllTi5e67E35q3f0e4BOtHWdmg8L9/iNcNuDjBHOCATwEXNOOuEVEREQyTrJdjSMTFj9A0AKWzLH3AF8DeobLfYA97l4fLu8gmIy1qXOWA+UA55xzTjJhSgR+uXX3ccuXDO2TpkgkFq+taX2fIZdGH4dEbmHlwg4dP6tgVooiETm5JDuP178lfK4HtgHTWjrAzD4JvO3uG8xsfFsDc/dFwCKAwsJCzRkmIiIiGS/Zpxova0fZY4GJZnYVkAOcDnwX6GVmXcNWr0FAbTvKFhEREck4yXY1fqWl7e7+nSbW3QLcEh4/HvgHd/+Mmf0MmErwZOMM4PG2hSwiIiKSmdryVOPfE4zHGgh8HhhJMHarZwvHNWUO8BUze5VgzNcDbTxeREREJCMlO8ZrEDDS3fcBmNlc4Al3vy6Zg919FbAq/LwVGN3WQEVEREQyXbItXv2BxElOD4frRERERCRJybZ4LQbWmtmj4fI1BHNwiYiIiEiSkn2q8Rtm9iTBrPUAM93919GFJSIiIpJ9ku1qBDgN2Ovu3wV2hO9cFBEREZEkJTudxG0ETzYOAx4EuhG8q3FsdKFlpwXPvpzuEI752B8WNb1hZbyz0y/c85tYzyciIpIuybZ4fQqYCLwH4O5v0PZpJEREREROaskOrj/s7m5mDmBmPSKMSdKs8fsZJQsk8w5GySgdfdeiiKRHsi1ePzWzHxK87qcMeA64P7qwRERERLJPqy1eZmbAT4CPAHsJxnnd6u7PRhybiIiISFZpNfEKuxj/293zASVbIiIiIu2UbFfjRjMrijQSERERkSyX7OD6McB1ZraN4MlGI2gMuziqwERERESyTYuJl5md4+5/AEpiikdEREQka7XW4vUYMNLdt5vZMnefEkNMIiIiIlmptTFelvB5aJSBiIiIiGS71hIvb+aziIiIiLRRa12NI8xsL0HL16nhZ/jz4PrTmzvQzHKA1UD38DwV7n5b+HLtR4A+wAbgs+5+uIPXISIiItLptdji5e5d3P10d+/p7l3Dzw3LzSZdoUPAx919BFAATDCzjwHfAha4+3nAu8DnUnAdIiIiIp1esvN4tZkH9oeL3cIfBz4OVITrHwKuiSoGERERkc4kssQLwMy6mFkl8DbBrPe/B/a4e324yw5gYDPHlpvZejNbv2vXrijDFBEREYlFpImXu7/v7gXAIGA0wfsekz12kbsXunthv379ogpRREREJDaRJl4N3H0PsBK4BOhlZg2D+gcBtXHEICIiIpJukSVeZtbPzHqFn08FrgS2ECRgU8PdZgCPRxWDiIiISGeS7Lsa22MA8JCZdSFI8H7q7ivMbDPwiJn9K/Br4IEIYxARERHpNCJLvNz9N8BHm1i/lWC8l4iIiMhJJZYxXiIiIiISbVejZKFfbt197PMlQ/ukMRJJm9fWJLffkEujjUPSamHlwg4dP6tgVooiEcksavESERERiYkSLxEREZGYKPESERERiYkSLxEREZGYKPESERERiYkSLxEREZGYKPESERERiYkSLxEREZGYKPESERERiYkSLxEREZGYKPESERERiYkSLxEREZGYKPESERERiYkSLxEREZGYRJZ4mdnZZrbSzDab2SYz+1K4/oNm9qyZvRL+7h1VDCIiIiKdSZQtXvXAV939QuBjwBfM7ELgZuB5dz8feD5cFhEREcl6kSVe7r7T3TeGn/cBW4CBwCTgoXC3h4BroopBREREpDOJZYyXmQ0GPgq8BPR3953hpjeB/s0cU25m681s/a5du+IIU0RERCRSkSdeZpYLLANucve9idvc3QFv6jh3X+Tuhe5e2K9fv6jDFBEREYlc1ygLN7NuBEnXEnf/ebj6LTMb4O47zWwA8HaUMaTLgmdfPvb5y1dekLJyP/aHRa3u86tzyttc7vIPvNr2Y7YFx0w8eh6XDO3T5uMly722pvV9hlwafRzSKS2sXNih42cVzEpRJCLxivKpRgMeALa4+3cSNi0HZoSfZwCPRxWDiIiISGcSZYvXWOCzQLWZVYbrvg7cCfzUzD4HbAemRRiDiIiISKdhwTCrzq2wsNDXr1+f7jDaJLGrMZWS6Wpsj/Z0NTbn7F6npqwsEUBdkpJy6qqUKJnZBncvbGqbZq4XERERiYkSLxEREZGYKPESERERiYkSLxEREZGYKPESERERiYkSLxEREZGYKPESERERiYkSLxEREZGYKPESERERiUmkL8k+2XRktvqOzkifypnnRUREJBpq8RIRERGJiRIvERERkZgo8RIRERGJiRIvERERkZgo8RIRERGJiRIvERERkZhENp2Emf0I+CTwtrvnhes+CPwEGAxsA6a5+7tRxRC1ZKePSGaqCE0HISIikv2ibPH6T2BCo3U3A8+7+/nA8+GyiIiIyEkhssTL3VcD7zRaPQl4KPz8EHBNVOcXERER6WziHuPV3913hp/fBPo3t6OZlZvZejNbv2vXrniiExEREYlQ2gbXu7sD3sL2Re5e6O6F/fr1izEyERERkWjE/a7Gt8xsgLvvNLMBwNsxn19i9vqeg8c+n93r1DRGIlnvtTXJ7Tfk0mjjkKy3sHJhh46fVTArRZFIJoq7xWs5MCP8PAN4PObzi4iIiKRNZImXmT0M/BIYZmY7zOxzwJ3AlWb2CnBFuCwiIiJyUoisq9Hdpzez6fKozikiIiLSmWnmehEREZGYKPESERERiYkSLxEREZGYKPESERERiUnc83hlvMQXYye+/Lqll1wvV3orIiIiqMVLREREJDZKvERERERiosRLREREJCZKvERERERiYu6e7hhaVVhY6OvXr4/0HCe89DThhbuJL3qW1NFLs6XT0ou0pRPTS7Y7PzPb4O6FTW1Ti5eIiIhITJR4iYiIiMREiZeIiIhITJR4iYiIiMREM9dLp5D4AIMG3UvGSHgIp1kaqC+dzAkPk7WRBvd3jFq8RERERGKSlhYvM5sAfBfoAvyHu9+Zjjgks6hVTE56ybSwgVrZJKt1pMWuM7TWxd7iZWZdgO8Dfw1cCEw3swvjjkNEREQkbunoahwNvOruW939MPAIMCkNcYiIiIjEKvaZ681sKjDB3a8Plz8LjHH32Y32KwfKw8VhwO9iDTSz9QX+mO4gsojqM3VUl6ml+kwt1Wdqncz1+WF379fUhk77VKO7LwIWpTuOTGRm65t7VYG0neozdVSXqaX6TC3VZ2qpPpuWjq7GWuDshOVB4ToRERGRrJaOxGsdcL6ZDTGzU4BrgeVpiENEREQkVrF3Nbp7vZnNBp4mmE7iR+6+Ke44spy6aFNL9Zk6qsvUUn2mluoztVSfTYh9cL2IiIjIyUoz14uIiIjERImXiIiISEyUeGURM5tgZr8zs1fN7OZ0x5PpzGybmVWbWaWZrU93PJnGzH5kZm+bWU3Cug+a2bNm9kr4u3c6Y8wkzdTnXDOrDe/RSjO7Kp0xZgozO9vMVprZZjPbZGZfCtfr/myHFupT92cTNMYrS4SvYnoZuBLYQfD06HR335zWwDKYmW0DCt39ZJ0AsEPMrBjYDyx297xw3beBd9z9zvA/B73dfU4648wUzdTnXGC/u9+dztgyjZkNAAa4+0Yz6wlsAK4BStH92WYt1Oc0dH+eQC1e2UOvYpJOxd1XA+80Wj0JeCj8/BDBX86ShGbqU9rB3Xe6+8bw8z5gCzAQ3Z/t0kJ9ShOUeGWPgcDrCcs70I3fUQ48Y2YbwldYScf1d/ed4ec3gf7pDCZLzDaz34RdkeoaayMzGwx8FHgJ3Z8d1qg+QffnCZR4iTRvnLuPBP4a+ELY1SMp4sE4B4116JgfAOcCBcBO4N/SGk2GMbNcYBlwk7vvTdym+7PtmqhP3Z9NUOKVPfQqphRz99rw99vAowTdudIxb4XjQRrGhbyd5ngymru/5e7vu/tR4H50jybNzLoRJAlL3P3n4Wrdn+3UVH3q/myaEq/soVcxpZCZ9QgHiWJmPYC/AmpaPkqSsByYEX6eATyexlgyXkOSEPoUukeTYmYGPABscffvJGzS/dkOzdWn7s+m6anGLBI+qnsPf34V0zfSG1HmMrOhBK1cELxaa6nqs23M7GFgPNAXeAu4DXgM+ClwDrAdmObuGjCehGbqczxBN44D24AbEsYoSTPMbBywBqgGjoarv04wLkn3Zxu1UJ/T0f15AiVeIiIiIjFRV6OIiIhITJR4iYiIiMREiZeIiIhITJR4iYiIiMREiZeIiIhITJR4iUhGMrOVZlbSaN1NZvaDZvZfZWaF8UQnItI0JV4ikqkeJpgoONG14XoRkU5JiZeIZKoK4BPhmxoaXs57FjDdzNab2SYzu72pA81sf8LnqWb2n+Hnfma2zMzWhT9jw/V/aWaV4c+vG95qICLSVl3THYCISHu4+ztmtpbgJeaPE7R2/RT4ZritC/C8mV3s7r9JstjvAgvc/X/N7BzgaWA48A/AF9z9xfBFwHUpvyAROSmoxUtEMllid2NDN+M0M9sI/Bq4CLiwDeVdAdxrZpUE7+07PUy0XgS+Y2ZfBHq5e32K4heRk4wSLxHJZI8Dl5vZSOA04B2C1qnL3f1i4Akgp4njEt+Vlrj9A8DH3L0g/Bno7vvd/U7geuBU4EUz+0gUFyMi2U+Jl4hkLHffD6wEfkTQ2nU68B7wJzPrT9AN2ZS3zGy4mX0A+FTC+meAGxsWzKwg/H2uu1e7+7eAdYASLxFpFyVeIpLpHgZGAA+7exVBF+NvgaUEXYRNuRlYAfwfsDNh/ReBQjP7jZltBj4frr/JzGrM7DfAEeDJ1F+GiJwMzN1b30tEREREOkwtXiIiIiIxUeIlIiIiEhMlXiIiIiIxUeIlIiIiEhMlXiIiIiIxUeIlIiIiEhMlXiIiIiIx+f+vUcfQbWVpNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "gaussian_0 = gaussian_generator(0, 1, 1000)\n",
    "gaussian_1 = gaussian_generator(5, 3, 1000)\n",
    "gaussian_2 = gaussian_generator(10, 5, 1000)\n",
    "\n",
    "utils.plot_gaussian_distributions(gaussian_0, gaussian_1, gaussian_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3296d4a",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "<img src=\"./assets/gaussian.png\" style=\"height: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78cc376",
   "metadata": {},
   "source": [
    "## Exercise 3: Binomial Generator\n",
    "\n",
    "### 3.1 Inverse CDF - Binomial\n",
    "\n",
    "If $X \\sim \\text{Binomial}(n,p)$, then its PDF is given by:\n",
    "\n",
    "$$P(X = k) = {n \\choose k}p^{k}(1-p)^{n-k}.$$\n",
    "\n",
    "Therefore, if $0 \\leq x \\leq n$, its CDF is given by:\n",
    "\n",
    "$$F(x) = P(X \\leq x) = P(X = 0) + P(X = 1) + \\ldots + P(X = \\lfloor x \\rfloor) = \\sum_{k=0}^{\\lfloor x \\rfloor} {n \\choose k}p^{k}(1-p)^{n-k}.$$\n",
    "\n",
    "Here, $\\lfloor x \\rfloor$ denotes the [floor function](https://en.wikipedia.org/wiki/Floor_and_ceiling_functions), which returns the greatest integer less than or equal to $x$. For example, $\\lfloor 2.9 \\rfloor = 2$ and $\\lfloor 1.2 \\rfloor = 1$. This function is necessary because the domain of $F$ is the entire set of real numbers, but $P(X = k)$ is non-zero only for positive integer values between 0 and $n$\n",
    "\n",
    "If $x > n$, then $F(x) = 1$. It is important to note that the expression for $F(x)$ can become complex and messy, and there is no closed-form expression for the inverse function $F^{-1}$ in this case. However, statistical libraries provide implementations of the inverse CDF using [generalized quantile functions](https://en.wikipedia.org/wiki/Cumulative_distribution_function#Inverse_distribution_function_.28quantile_function.29). You can refer to [scipy.stats.binom](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html) for an example of a library that implements these functions. In particular, the `scipy.stats.binom.ppf` function is what you need. Since the `binom` class is already imported, you can use `help(binom)` to explore its parameters and functions. The function you will need is located in the \"Methods\" section: `ppf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1819bfed",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def inverse_cdf_binomial(y, n, p):\n",
    "    \"\"\"\n",
    "    Calculates the inverse cumulative distribution function (CDF) of a binomial distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - y (float or ndarray): The probability or array of probabilities.\n",
    "    - n (int): The number of trials in the binomial distribution.\n",
    "    - p (float): The probability of success in each trial.\n",
    "\n",
    "    Returns:\n",
    "    - x (float or ndarray): The corresponding value(s) from the binomial distribution that correspond to the given probability/ies.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "#     x = binom(n, p).ppf(y) # @OMIT\n",
    "    x = binom.ppf(y, n, p)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6c769c3",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse of Binomial CDF with n 15 and p 0.9 for value 1e-10: 3.000\n",
      "Inverse of Binomial CDF with n 15 and p 0.5 for value 0: -1.0\n",
      "Inverse of Binomial CDF with n 20 and p 0.2 for value 0.4: 3.000\n",
      "Inverse of Binomial CDF with n 15 and p 0.5 for value 1: 15.0\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "print(f\"Inverse of Binomial CDF with n {15} and p {0.9} for value {1e-10}: {inverse_cdf_binomial(1e-10, 15, 0.9):.3f}\")\n",
    "print(f\"Inverse of Binomial CDF with n {15} and p {0.5} for value {0}: {inverse_cdf_binomial(0, 15, 0.5)}\")\n",
    "print(f\"Inverse of Binomial CDF with n {20} and p {0.2} for value {0.4}: {inverse_cdf_binomial(0.4, 20, 0.2):.3f}\")\n",
    "print(f\"Inverse of Binomial CDF with n {15} and p {0.5} for value {1}: {inverse_cdf_binomial(1, 15, 0.5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fa672",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Inverse of Binomial CDF with n 15 and p 0.9 for value 1e-10: 3.000\n",
    "Inverse of Binomial CDF with n 15 and p 0.5 for value 0: -1.0\n",
    "Inverse of Binomial CDF with n 20 and p 0.2 for value 0.4: 3.000\n",
    "Inverse of Binomial CDF with n 15 and p 0.5 for value 1: 15.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc766a4",
   "metadata": {},
   "source": [
    "### 3.2 Binomial Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea27b236",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def binomial_generator(n, p, num_samples):\n",
    "    \"\"\"\n",
    "    Generates an array of binomially distributed random numbers.\n",
    "\n",
    "    Args:\n",
    "        n (int): The number of trials in the binomial distribution.\n",
    "        p (float): The probability of success in each trial.\n",
    "        num_samples (int): The number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        array: An array of binomially distributed random numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate an array with num_samples elements that distribute uniformally between 0 and 1\n",
    "    u = uniform_generator(0, 1, num_samples)\n",
    "\n",
    "    # Use the uniform-distributed sample to generate binomial-distributed data\n",
    "    # Hint: You need to sample from the inverse of the CDF of the distribution you are generating\n",
    "    array = inverse_cdf_binomial(u, n, p)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e06e9f1e",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "binomial_0 = binomial_generator(12, 0.4, 1000)\n",
    "binomial_1 = binomial_generator(15, 0.5, 1000)\n",
    "binomial_2 = binomial_generator(25, 0.8, 1000)\n",
    "\n",
    "utils.plot_binomial_distributions(binomial_0, binomial_1, binomial_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d2b52",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "<img src=\"./assets/binomial2.png\" style=\"height: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664454c7",
   "metadata": {},
   "source": [
    "# Section 2: Building a Dog Breed Classifier using Naive Bayes\n",
    "\n",
    "In this section, you will utilize the generator functions to create features for a synthetic dataset containing information about three different dog breeds. Once you have prepared the dataset, your task is to implement the Naive Bayes algorithm to classify the dogs accurately based on their features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e560546",
   "metadata": {},
   "source": [
    "## Section 2.1: Generating the Dataset\n",
    "\n",
    "In this section, we will generate a dataset that consists of four features for each dog:\n",
    "\n",
    "- `height` in centimeters, which follows a Gaussian distribution.\n",
    "- `weight` in kilograms, which follows a Gaussian distribution.\n",
    "- `bark_days`, representing the number of days (out of 30) that the dog barks. It follows a Binomial distribution with `n = 30`.\n",
    "- `ear_head_ratio`, which is the ratio between the length of the ears and the length of the head. It follows a Uniform distribution.\n",
    "\n",
    "We will generate synthetic data using the generator functions defined earlier to create a diverse and representative dataset for our dog breed classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45961a80",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "FEATURES = [\"height\", \"weight\", \"bark_days\", \"ear_head_ratio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4064e165",
   "metadata": {},
   "source": [
    "Since the features follow different distributions and each one of these has different parameters you will create a `dataclass` for each one so you have an easy way of saving parameters. If you haven't used dataclasses before, don't worry, they are nothing complicated, you can think of them as containers for data in which you can access each variable by using the dot notation. So for example if you have:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class my_data_class:\n",
    "    my_var: str\n",
    "        \n",
    "foo = my_data_class(my_var=\"Hello World\")\n",
    "```\n",
    "\n",
    "You can access the information of `my_var` from `foo` by using the syntax `foo.my_var`, which should be equal to \"Hello World\" in this example.\n",
    "\n",
    "Dataclasses were introduced in Python 3.7 and are an excellent way of storing data (notice that it is a good practice when using them to specify the types of the data they store) so if you didn't know about them, you can start using them in your projects too. Also don't worry about the `__repr__` method, this only controls how many decimal points are printed out when you print an object of these dataclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c34979c7",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class params_gaussian:\n",
    "    mu: float\n",
    "    sigma: float\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"params_gaussian(mu={self.mu:.3f}, sigma={self.sigma:.3f})\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class params_binomial:\n",
    "    n: int\n",
    "    p: float\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"params_binomial(n={self.n:.3f}, p={self.p:.3f})\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class params_uniform:\n",
    "    a: int\n",
    "    b: int\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"params_uniform(a={self.a:.3f}, b={self.b:.3f})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546474bf",
   "metadata": {},
   "source": [
    "Now that you have a place to store information about the parameters for different probability distributions you will define a dictionary that has the information for every breed of dogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e9b5468",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "breed_params = {\n",
    "    0: {\n",
    "        \"height\": params_gaussian(mu=35, sigma=1.5),\n",
    "        \"weight\": params_gaussian(mu=20, sigma=1),\n",
    "        \"bark_days\": params_binomial(n=30, p=0.8),\n",
    "        \"ear_head_ratio\": params_uniform(a=0.6, b=0.1)\n",
    "    },\n",
    "    \n",
    "    1: {\n",
    "        \"height\": params_gaussian(mu=30, sigma=2),\n",
    "        \"weight\": params_gaussian(mu=25, sigma=5),\n",
    "        \"bark_days\": params_binomial(n=30, p=0.5),\n",
    "        \"ear_head_ratio\": params_uniform(a=0.2, b=0.5)\n",
    "    },\n",
    "    \n",
    "    2: {\n",
    "        \"height\": params_gaussian(mu=40, sigma=3.5),\n",
    "        \"weight\": params_gaussian(mu=32, sigma=3),\n",
    "        \"bark_days\": params_binomial(n=30, p=0.3),\n",
    "        \"ear_head_ratio\": params_uniform(a=0.1, b=0.3)\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626fbe23",
   "metadata": {},
   "source": [
    "With the parameters and distributions for each breed defined you will generate the dataset. For this the `generate_data_for_breed` is provided for you. Notice that this function uses a `match` statement which was introduced in Python 3.10 (the same version available in this environment) which allows this function to be written in a clear manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b94140b",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>bark_days</th>\n",
       "      <th>ear_head_ratio</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>39.697810</td>\n",
       "      <td>31.740980</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.193120</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>36.710641</td>\n",
       "      <td>21.140427</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.163527</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>34.726930</td>\n",
       "      <td>19.817954</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.386113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>32.324884</td>\n",
       "      <td>30.812210</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.463242</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>37.691499</td>\n",
       "      <td>21.794333</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.118190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>36.688852</td>\n",
       "      <td>21.125901</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.165052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>30.844078</td>\n",
       "      <td>27.110196</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.399051</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>38.616784</td>\n",
       "      <td>30.814387</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.169269</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>44.655532</td>\n",
       "      <td>35.990456</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.281653</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>35.209095</td>\n",
       "      <td>20.139397</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.322284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         height     weight  bark_days  ear_head_ratio  breed\n",
       "2836  39.697810  31.740980        9.0        0.193120      2\n",
       "1002  36.710641  21.140427       26.0        0.163527      0\n",
       "1075  34.726930  19.817954       24.0        0.386113      0\n",
       "1583  32.324884  30.812210       18.0        0.463242      1\n",
       "248   37.691499  21.794333       28.0        0.118190      0\n",
       "814   36.688852  21.125901       26.0        0.165052      0\n",
       "1407  30.844078  27.110196       16.0        0.399051      1\n",
       "3376  38.616784  30.814387        8.0        0.169269      2\n",
       "2700  44.655532  35.990456       12.0        0.281653      2\n",
       "533   35.209095  20.139397       24.0        0.322284      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_data_for_breed(breed, features, n_samples, params):\n",
    "    \"\"\"\n",
    "    Generate synthetic data for a specific breed of dogs based on given features and parameters.\n",
    "\n",
    "    Parameters:\n",
    "        - breed (str): The breed of the dog for which data is generated.\n",
    "        - features (list[str]): List of features to generate data for (e.g., \"height\", \"weight\", \"bark_days\", \"ear_head_ratio\").\n",
    "        - n_samples (int): Number of samples to generate for each feature.\n",
    "        - params (dict): Dictionary containing parameters for each breed and its features.\n",
    "\n",
    "    Returns:\n",
    "        - df (pandas.DataFrame): A DataFrame containing the generated synthetic data.\n",
    "            The DataFrame will have columns for each feature and an additional column for the breed.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for feature in features:\n",
    "        match feature:\n",
    "            case \"height\" | \"weight\":\n",
    "                df[feature] = gaussian_generator(params[breed][feature].mu, params[breed][feature].sigma, n_samples)\n",
    "                \n",
    "            case \"bark_days\":\n",
    "                df[feature] = binomial_generator(params[breed][feature].n, params[breed][feature].p, n_samples)\n",
    "                                       \n",
    "            case \"ear_head_ratio\":\n",
    "                df[feature] = uniform_generator(params[breed][feature].a, params[breed][feature].b, n_samples)    \n",
    "    \n",
    "    df[\"breed\"] = breed\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "# Generate data for each breed\n",
    "df_0 = generate_data_for_breed(breed=0, features=FEATURES, n_samples=1200, params=breed_params)\n",
    "df_1 = generate_data_for_breed(breed=1, features=FEATURES, n_samples=1350, params=breed_params)\n",
    "df_2 = generate_data_for_breed(breed=2, features=FEATURES, n_samples=900, params=breed_params)\n",
    "\n",
    "# Concatenate all breeds into a single dataframe\n",
    "df_all_breeds = pd.concat([df_0, df_1, df_2]).reset_index(drop=True)\n",
    "\n",
    "# Shuffle the data\n",
    "df_all_breeds = df_all_breeds.sample(frac = 1)\n",
    "\n",
    "# Print the dataframe\n",
    "df_all_breeds.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773730b",
   "metadata": {},
   "source": [
    "All that is left is to divide the generated dataset into training and testing splits. You will use the 70% of the data for training and the remaining 30% for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "722e8ec7",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# Define a 70/30 training/testing split\n",
    "split = int(len(df_all_breeds)*0.7)\n",
    "\n",
    "# Do the split\n",
    "df_train = df_all_breeds[:split].reset_index(drop=True)\n",
    "df_test = df_all_breeds[split:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f31e13a",
   "metadata": {},
   "source": [
    "## Section 2.2 Brief Recap on Naive Bayes Algorithm\n",
    "\n",
    "Let's recap how the Naive Bayes algorithm works and formalize the notation. \n",
    "\n",
    "Let $X$ be a set of training data. An element $x \\in X$ is a vector in the form $x = (x_1, x_2, \\ldots, x_n)$, where $n$ is the number of attributes of each sample. For instance, $X$ can be a set of 100 dog breeds, and each dog breed might have 3 attributes, such as ear head ratio, weight, and height. So, $X = \\{ \\text{dog}_1, \\text{dog}_2, \\ldots, \\text{dog}_{100} \\}$, and each dog breed, for instance, dog 5, will be represented as a 3-dimensional vector: $\\text{dog}_5 = (\\text{ear head ratio}_{\\text{dog}_5}, \\text{weight}_{\\text{dog}_5}, \\text{height}_{\\text{dog}_5})$.\n",
    "\n",
    "Suppose that there are $m$ classes $C_1, C_2, \\ldots, C_m$. Using the same example above, suppose there are $m = 5$ different types of dog breeds in the training data. The idea is to predict the class of a sample $x \\in X$ by looking at its attributes. Naive Bayes does so by computing the posterior probabilities of a sample belonging to class $C_i$, i.e., it computes\n",
    "\n",
    "$$\\mathbf P(C_i \\mid x), \\quad i = 1, \\ldots, m.$$\n",
    "\n",
    "The predicted class is the $C_i$ with the highest probability. More formally, considering the set of every posterior probability of a given sample, what Naive Bayes computes is:\n",
    "\n",
    "$$\\text{predicted class for } x = \\arg \\max \\left\\{ \\mathbf P(C_1 \\mid x), \\mathbf P(C_2 \\mid x), \\ldots, \\mathbf P(C_m \\mid x) \\right\\}$$\n",
    "\n",
    "So, if the highest value is $\\mathbf P(C_4 \\mid x)$, then $\\arg \\max \\left\\{\\mathbf  P(C_1 \\mid x), \\mathbf P(C_2 \\mid x), \\ldots, \\mathbf P(C_m \\mid x) \\right\\} = 4$.\n",
    "\n",
    "To compute the posterior probability $\\mathbf P(C_i \\mid x)$, we use the Bayes' Theorem:\n",
    "\n",
    "$$\\mathbf P(C_i \\mid x) = \\frac{\\mathbf P(x \\mid C_i)\\mathbf P(C_i)}{\\mathbf P(x)}.$$\n",
    "\n",
    "In this equation we use $\\mathbf P(x \\mid C_i)$ generically to indicate the distriution of $X|C_i$. If $X$ is a contiuous random variable, $\\mathbf P(x|C_i)$ should be interpreted as $f_{X|C_i}(x)$. Note that $\\mathbf P(x)$ is positive and constant for every class $C_i$, therefore, to maximize $\\mathbf P(C_i \\mid x)$, it is sufficient to maximize $\\mathbf P(x \\mid C_i)\\mathbf P(C_i)$. The probabilities $\\mathbf P(C_i)$ are called the class prior probabilities, and they denote how likely a random sample from $X$ (without knowing any of its attributes) is to belong to each class. This value is usually not known and can be estimated from the training set by computing the proportion of each class in the training set. If the training set is too small, it is common to assume that each class is equally likely, i.e., $\\mathbf P(C_1) = \\mathbf P(C_2) = \\ldots = \\mathbf P(C_m)$, thus only maximizing $\\mathbf P(x \\mid C_i)$ remains. We will work with the more general case here.\n",
    "\n",
    "In general, it would be computationally expensive to compute $\\mathbf P(x \\mid C_i)$ for each $x$ and each class, this is why a **naive** assumption of **class-conditional independence** is made. This assumption states that each attribute is independent of each other attribute within each class. It is a strong assumption. For example, in our dog breed example, it would mean that for a specific type of dog breed, there is no correlation between its weight, height, and ear head ratio.\n",
    "\n",
    "Assuming class-conditional independence, for an $x = (x_1, \\ldots x_n)$ in $X$:\n",
    "\n",
    "$$\\mathbf P(x \\mid C_i) = \\mathbf P(x_1 \\mid C_i) \\cdot \\mathbf P(x_2 \\mid C_i) \\cdot \\ldots \\cdot \\mathbf P(x_n \\mid C_i) = \\prod_{k = 1}^{n} \\mathbf P(x_k \\mid C_i).$$\n",
    "\n",
    "The probabilities $\\mathbf P(x_k \\mid C_i)$ can be estimated from the training data. The computation of $\\mathbf P(x_k \\mid C_i)$ depends on whether $x_k$ is categorical or not.\n",
    "\n",
    "- If $x_k$ is categorical, then $\\mathbf P(x_k \\mid C_i)$ is the number of samples in $X$ that have attribute $x_k$ divided by the number of samples in class $C_i$.\n",
    "\n",
    "- If $x_k$ is continuous-valued or discrete-valued, we need to make an assumption about its distribution and estimate its parameters using the training data. For instance, if $x_k$ is continuous-valued, we can assume that $\\mathbf P(x_k \\mid C_i)$ follows a Gaussian distribution with parameters $\\mu_{C_i}$ and $\\sigma_{C_i}$. Therefore, we need to estimate $\\mu$ and $\\sigma$ from the training data, and then $\\mathbf P(x_k \\mid C_i) = \\text{PDF}_{\\text{gaussian}}(x_k, \\mu_{C_i}, \\sigma_{C_i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86787fc",
   "metadata": {},
   "source": [
    "## Exercise 4: PDF for Distributions\n",
    "\n",
    "To calculate the probabilities of predicting each class using Naive Bayes, you need to compute the prior probabilities. Although you already know the prior for each feature, you still need a way to compute the probability. In the next exercise, you are required to write a function that takes a value `x` and the relevant parameters and returns the value of the Probability Density Function (`PDF`) for each distribution.\n",
    "\n",
    "You can choose to implement this function on your own or utilize the implementation from the [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html) module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30569fed",
   "metadata": {},
   "source": [
    "### 4.1 Uniform PDF\n",
    "\n",
    "If $X \\sim \\text{Uniform}(a,b)$, then the PDF for $X$ is given by:\n",
    "\n",
    "\n",
    "$$f_X(x) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{b-a}, \\quad \\text{if } x \\in [a,b]. \\\\\n",
    "0, \\quad \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4930247d",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def pdf_uniform(x, a, b):\n",
    "    \"\"\"\n",
    "    Calculates the probability density function (PDF) for a uniform distribution between 'a' and 'b' at a given point 'x'.\n",
    "\n",
    "    Args:\n",
    "        x (float): The value at which the PDF is evaluated.\n",
    "        a (float): The lower bound of the uniform distribution.\n",
    "        b (float): The upper bound of the uniform distribution.\n",
    "\n",
    "    Returns:\n",
    "        float: The PDF value at the given point 'x'. Returns 0 if 'x' is outside the range [a, b].\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    pdf = 1 / (b - a) if (x >= a and x<= b) else 0 # @REPLACE EQUALS None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f894d948",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform PDF with a=0 and b=5 for value 1e-10: 0.200\n",
      "Uniform PDF with a=20 and b=25 for value 5: 0.000\n",
      "Uniform PDF with a=2 and b=10 for value 5.4: 0.125\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "print(f\"Uniform PDF with a={0} and b={5} for value {1e-10}: {pdf_uniform(1e-10, 0, 5):.3f}\")\n",
    "print(f\"Uniform PDF with a={20} and b={25} for value {5}: {pdf_uniform(5, 20, 25):.3f}\")\n",
    "print(f\"Uniform PDF with a={2} and b={10} for value {5.4}: {pdf_uniform(5.4, 2, 10):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c9f5e",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Uniform PDF with a=0 and b=5 for value 1e-10: 0.200\n",
    "Uniform PDF with a=20 and b=25 for value 5: 0.000\n",
    "Uniform PDF with a=2 and b=10 for value 5.4: 0.125\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0bc5b7",
   "metadata": {},
   "source": [
    "### 4.2 Gaussian PDF\n",
    "\n",
    "You will need to implement the PDF for the Gaussian Distribution. The PDF for $X$ if $X \\sim \\text{Normal}(\\mu,\\sigma)$ is given by:\n",
    "\n",
    "$$f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2be18fe6",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def pdf_gaussian(x, mu, sigma):\n",
    "    \"\"\"\n",
    "    Calculate the probability density function (PDF) of a Gaussian distribution at a given value.\n",
    "\n",
    "    Args:\n",
    "        x (float or array-like): The value(s) at which to evaluate the PDF.\n",
    "        mu (float): The mean of the Gaussian distribution.\n",
    "        sigma (float): The standard deviation of the Gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "        float or ndarray: The PDF value(s) at the given point(s) x.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    pdf = (1 / (np.sqrt(2 * np.pi) * sigma) * np.exp(-(1 / 2) * ((x - mu) / sigma) ** 2))  \n",
    "    \n",
    "    # pdf = norm.pdf(x, mean, std) # @OMIT\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32bb2b1e",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian PDF with mu=15 and sigma=5 for value 10: 0.048\n",
      "Gaussian PDF with mu=15 and sigma=5 for value 0: 0.001\n",
      "Gaussian PDF with mu=20 and sigma=0.5 for value 20: 0.798\n",
      "Gaussian PDF with mu=15 and sigma=5 for value 1: 0.002\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "print(f\"Gaussian PDF with mu={15} and sigma={5} for value {10}: {pdf_gaussian(10, 15, 5):.3f}\")\n",
    "print(f\"Gaussian PDF with mu={15} and sigma={5} for value {0}: {pdf_gaussian(0, 15, 5):.3f}\")\n",
    "print(f\"Gaussian PDF with mu={20} and sigma={0.5} for value {20}: {pdf_gaussian(20, 20, 0.5):.3f}\")\n",
    "print(f\"Gaussian PDF with mu={15} and sigma={5} for value {1}: {pdf_gaussian(1, 15, 5):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e3a5f",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Gaussian PDF with mu=15 and sigma=5 for value 10: 0.048\n",
    "Gaussian PDF with mu=15 and sigma=5 for value 0: 0.001\n",
    "Gaussian PDF with mu=20 and sigma=0.5 for value 20: 0.798\n",
    "Gaussian PDF with mu=15 and sigma=5 for value 1: 0.002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f27cce",
   "metadata": {},
   "source": [
    "### 4.3 Binomial Probability Mass Function (PMF)\n",
    "\n",
    "For the binomial distribution, since it is a discrete distribution, we will be using the Probability Mass Function (PMF) instead of the Probability Density Function (PDF). However, for consistency, the graded function should still be named `pdf_binomial`.\n",
    "\n",
    "Remember that if we have a random variable X following a binomial distribution with parameters n and p, its PMF is given by:\n",
    "$$p_X(x) = {n \\choose x}  p^x  (1-p)^{n-x},\\quad x=1,2,\\ldots, n$$\n",
    "\n",
    "Here, you can calculate the combination ${n \\choose x}$ using either the definition: ${n \\choose x} = \\frac{n!}{x!(n-x)!}$, utilizing the `math.factorial` function, or you can use the `scipy.special.comb` function to obtain the combination. You can also refer to the `binom` documentation to find any other relevant functions that may assist you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64167eef",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def pdf_binomial(x, n, p):\n",
    "    \"\"\"\n",
    "    Calculate the probability mass function (PMF) of a binomial distribution at a specific value.\n",
    "\n",
    "    Args:\n",
    "        x (int): The value at which to evaluate the PMF.\n",
    "        n (int): The number of trials in the binomial distribution.\n",
    "        p (float): The probability of success for each trial.\n",
    "\n",
    "    Returns:\n",
    "        float: The probability mass function (PMF) of the binomial distribution at the specified value.\n",
    "    \"\"\"\n",
    "\n",
    "    pdf = binom(n, p).pmf(x)\n",
    "    \n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e15b91ec",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binomial PMF with n=15 and p=0.9 for value 15: 0.206\n",
      "Binomial PMF with n=30 and p=0.5 for value 15: 0.144\n",
      "Binomial PMF with n=20 and p=0.9 for value 15: 0.032\n",
      "Binomial PMF with n=15 and p=0.5 for value 20: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "print(f\"Binomial PMF with n={15} and p={0.9} for value {15}: {pdf_binomial(15, 15, 0.9):.3f}\")\n",
    "print(f\"Binomial PMF with n={30} and p={0.5} for value {15}: {pdf_binomial(15, 30, 0.5):.3f}\")\n",
    "print(f\"Binomial PMF with n={20} and p={0.9} for value {15}: {pdf_binomial(15, 20, 0.9):.3f}\")\n",
    "print(f\"Binomial PMF with n={15} and p={0.5} for value {20}: {pdf_binomial(20, 15, 0.5):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50dbf59",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Binomial PMF with n=15 and p=0.9 for value 15: 0.206\n",
    "Binomial PMF with n=30 and p=0.5 for value 15: 0.144\n",
    "Binomial PMF with n=20 and p=0.9 for value 15: 0.032\n",
    "Binomial PMF with n=15 and p=0.5 for value 20: 0.000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb2dc5",
   "metadata": {},
   "source": [
    "## 2.3 Estimated Parameters\n",
    "\n",
    "Now that you have the `PDF`s ready you need a way of estimating the parameters of the distributions for the features in the training split, this translates to estimating:\n",
    "\n",
    "- `mu` and `sigma` for the `height` feature\n",
    "- `mu` and `sigma` for the `weight` feature \n",
    "- `n` and `p` for the `bark_days` feature \n",
    "- `a` and `b` for the `ear_head_ratio` feature\n",
    "\n",
    "\n",
    "Since the interpretation and way of computing these parameters has not been covered in the lectures, the assignment provides functions that can accomplish this for you. These have already been imported into this environment and are called:\n",
    "\n",
    "- `estimate_gaussian_params`\n",
    "- `estimate_binomial_params`\n",
    "- `estimate_uniform_params`\n",
    "\n",
    "All of these functions work in the same way. They expect an array of numbers (a numpy array, a pandas series or a regular python list) and will return the relevant parameters depending on the distribution selected. An example of how to use these functions can be seen by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "febc3031",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian:\n",
      "mu = 24.583 and sigma = 7.232 for sample: [26.31 32.45 14.99]\n",
      "\n",
      "Binomial:\n",
      "n = 30 and p = 0.480 for sample: [ 9 26 18 14  5]\n",
      "\n",
      "Uniform:\n",
      "a = 0.070 and b = 0.900 for sample: [0.9  0.26 0.18 0.07 0.5 ]\n"
     ]
    }
   ],
   "source": [
    "m, s = estimate_gaussian_params(np.array([26.31, 32.45, 14.99]))\n",
    "print(f\"Gaussian:\\nmu = {m:.3f} and sigma = {s:.3f} for sample: {np.array([26.31, 32.45, 14.99])}\\n\")\n",
    "\n",
    "n, p = estimate_binomial_params(np.array([9, 26, 18, 14, 5]))\n",
    "print(f\"Binomial:\\nn = {n} and p = {p:.3f} for sample: {np.array([9, 26, 18, 14, 5])}\\n\")\n",
    "\n",
    "a, b = estimate_uniform_params(np.array([0.9, 0.26, 0.18, 0.07, 0.5]))\n",
    "print(f\"Uniform:\\na = {a:.3f} and b = {b:.3f} for sample: {np.array([0.9, 0.26, 0.18, 0.07, 0.5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618af71",
   "metadata": {},
   "source": [
    "## Exercise 5: Computing parameters out of the training data\n",
    "\n",
    "Now you have all the pieces you need to code the `compute_training_parameters` below. This function should receive a dataframe with the same structure as the one you generated at the beginning of this section and return two dictionaries: \n",
    "\n",
    "- The first one (`params_dict`) should contain the estimated parameters of each feature for every breed. To be more concrete the first level should have the breeds (encoded as integers) as keys and the values should be another dictionary with the name of each feature as keys and with the estimated parameters saved within the relevant dataclass as values. To make this clearer, the end dictionary should end up looking like this:\n",
    "\n",
    "```\n",
    "{0: {'bark_days': params_dataclass(param1=x11, param2=x12),\n",
    "     'ear_head_ratio': params_dataclass(param1=x21, param2=x22),\n",
    "     'height': params_dataclass(param1=x31, param2=x32),\n",
    "     'weight': params_dataclass(param1=x42, param2=x42)},\n",
    " 1: ...     \n",
    "}\n",
    "```\n",
    "\n",
    "- The second one (`probs_dict`) should include the proportion of data belonging to each breed. Notice that all values should sum up to 1. You can use Python's built-in `round` function to avoid very long floats but this is up to you and your grade will not be affected by this. This dict should look like this:\n",
    "\n",
    "```\n",
    "{0: 0.25, 1: 0.5, 2: 0.25}    \n",
    "```\n",
    "\n",
    "Notice that some structure has been pre-defined to help you out with the implementation. This structure uses a `match` statement but feel free to use any other way of coding this function (there are many ways to do it!). As long as the returning dictionaries contain the correct information, the implementation details won't affect your grade. As a reference for the `match` statement, you can access [here](https://peps.python.org/pep-0636/). Feel free to use `if-else` statement if you feel more comfortable with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4023c2c1",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_training_params(df, features):\n",
    "    \"\"\"\n",
    "    Computes the estimated parameters for training a model based on the provided dataframe and features.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The dataframe containing the training data.\n",
    "        features (list): A list of feature names to consider.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two dictionaries:\n",
    "            - params_dict (dict): A dictionary that contains the estimated parameters for each breed and feature.\n",
    "            - probs_dict (dict): A dictionary that contains the proportion of data belonging to each breed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dict that should contain the estimated parameters\n",
    "    params_dict = {}\n",
    "    \n",
    "    # Dict that should contain the proportion of data belonging to each class\n",
    "    probs_dict = {}\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Loop over the breeds\n",
    "    for breed in df_train[\"breed\"]:\n",
    "        \n",
    "        # Slice the original df to only include data for the current breed and the feature columns\n",
    "        # For reference in slicing with pandas, you can use the df_breed.groupby function followed by .get_group\n",
    "        # or you can use the syntax df[df['breed'] == group]\n",
    "        df_breed = df_train[df_train[\"breed\"] == breed][features]\n",
    "        \n",
    "        # Save the probability of each class (breed) in the probabilities dict\n",
    "        # You can find the number of rows in a dataframe by using len(dataframe)\n",
    "        probs_dict[breed] = len(df_breed)/len(df) \n",
    "        \n",
    "        # Initialize the inner dict\n",
    "        inner_dict = {} #@KEEP\n",
    "        \n",
    "        # Loop over the columns of the sliced dataframe\n",
    "        # You can get the columns of a dataframe like this: dataframe.columns\n",
    "        for feature in df_breed.columns:\n",
    "        \n",
    "            match feature:\n",
    "                case \"height\" | \"weight\": \n",
    "                    # Estimate parameters depending on the distribution of the current feature\n",
    "                    # and save them in the corresponding dataclass object\n",
    "                    mu = df_breed[feature].mean()\n",
    "                    sigma = df_breed[feature].std()\n",
    "                    params = params_gaussian(mu=mu, sigma=sigma)\n",
    "                    \n",
    "                case \"bark_days\":\n",
    "                    # Estimate parameters depending on the distribution of the current feature\n",
    "                    # and save them in the corresponding dataclass object\n",
    "                    n = df_breed[feature].max()\n",
    "                    p = df_breed[feature].mean() / n\n",
    "                    params = params_binomial(n=n, p=p)\n",
    "                    \n",
    "                case \"ear_head_ratio\":\n",
    "                    # Estimate parameters depending on the distribution of the current feature\n",
    "                    # and save them in the corresponding dataclass object\n",
    "                    a = df_breed[feature].min()\n",
    "                    b = df_breed[feature].max()\n",
    "                    params = params_uniform(a=a, b=b)\n",
    "            \n",
    "            # Save the dataclass object within the inner dict\n",
    "            inner_dict[feature] = params\n",
    "        \n",
    "        # Save inner dict within outer dict\n",
    "        params_dict[breed] = inner_dict\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return params_dict, probs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3be3e2df",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution parameters for training split:\n",
      "\n",
      "{0: {'bark_days': params_binomial(n=30.000, p=0.801),\n",
      "     'ear_head_ratio': params_uniform(a=0.100, b=0.597),\n",
      "     'height': params_gaussian(mu=35.030, sigma=1.519),\n",
      "     'weight': params_gaussian(mu=20.020, sigma=1.013)},\n",
      " 1: {'bark_days': params_binomial(n=24.000, p=0.622),\n",
      "     'ear_head_ratio': params_uniform(a=0.201, b=0.500),\n",
      "     'height': params_gaussian(mu=29.971, sigma=2.011),\n",
      "     'weight': params_gaussian(mu=24.927, sigma=5.028)},\n",
      " 2: {'bark_days': params_binomial(n=18.000, p=0.493),\n",
      "     'ear_head_ratio': params_uniform(a=0.101, b=0.300),\n",
      "     'height': params_gaussian(mu=39.814, sigma=3.575),\n",
      "     'weight': params_gaussian(mu=31.841, sigma=3.064)}}\n",
      "\n",
      "Probability of each class for training split:\n",
      "\n",
      "{0: 0.3461697722567288, 1: 0.39337474120082816, 2: 0.26045548654244305}\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "train_params, train_class_probs = compute_training_params(df_train, FEATURES)\n",
    "\n",
    "print(\"Distribution parameters for training split:\\n\")\n",
    "pp.pprint(train_params)\n",
    "print(\"\\nProbability of each class for training split:\\n\")\n",
    "pp.pprint(train_class_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078d7688",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Distribution parameters for training split:\n",
    "\n",
    "{0: {'bark_days': params_binomial(n=30.000, p=0.801),\n",
    "     'ear_head_ratio': params_uniform(a=0.100, b=0.597),\n",
    "     'height': params_gaussian(mu=35.030, sigma=1.518),\n",
    "     'weight': params_gaussian(mu=20.020, sigma=1.012)},\n",
    " 1: {'bark_days': params_binomial(n=30.000, p=0.498),\n",
    "     'ear_head_ratio': params_uniform(a=0.201, b=0.500),\n",
    "     'height': params_gaussian(mu=29.971, sigma=2.010),\n",
    "     'weight': params_gaussian(mu=24.927, sigma=5.025)},\n",
    " 2: {'bark_days': params_binomial(n=30.000, p=0.296),\n",
    "     'ear_head_ratio': params_uniform(a=0.101, b=0.300),\n",
    "     'height': params_gaussian(mu=39.814, sigma=3.572),\n",
    "     'weight': params_gaussian(mu=31.841, sigma=3.061)}}\n",
    "\n",
    "Probability of each class for training split:\n",
    "\n",
    "{0: 0.346, 1: 0.393, 2: 0.26}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e2308",
   "metadata": {},
   "source": [
    "## Exercise 6: Compute the Probability of X given the Breed -> $P(x \\mid C_{i})$\n",
    "\n",
    "To code a Naive Bayes classifier, you will assume **class-conditional independence** for a given $\\boldsymbol x = (x_1, \\ldots, x_n)$ in $\\boldsymbol X$. With this assumption, you can compute the probability of $x$ given the class using the following expression:\n",
    "\n",
    "$$\\mathbf P(\\boldsymbol x \\mid C_{i}) = \\mathbf P(x_1 \\mid C_i) \\cdot \\mathbf P(x_2 \\mid C_i) \\cdot \\ldots \\cdot \\mathbf P(x_n \\mid C_i) = \\prod_{k = 1}^{n} \\mathbf P(x_k \\mid C_i).$$\n",
    "\n",
    "The probabilities $\\mathbf P(x_k \\mid C_i)$ can be estimated from the training tuples.\n",
    "\n",
    "If $x_k$ is continuous-valued or discrete-valued, you need to make an assumption about its distribution and estimate its parameters using the training set. For example, if $x_k$ is continuous-valued, it is often assumed that $\\mathbf P(x_k \\mid C_i)$ follows a Gaussian distribution with parameters $\\mu_{C_i}$ and $\\sigma_{C_i}$. Therefore, you need to estimate $\\mu$ and $\\sigma$ from the training set, and then $\\mathbf P(x_k \\mid C_i) = \\text{PDF}_{\\text{gaussian}}(x_k,\\mu_{C_i},\\sigma_{C_i})$.\n",
    "\n",
    "In this case, you already know the true distributions for every feature, so you just need to compute the appropriate `PDF` for each feature by passing the estimated parameters of that feature to the corresponding `PDF` computation function.\n",
    "\n",
    "Complete the `prob_of_X_given_C` function below. This function takes the following parameters:\n",
    "- `X`: a list containing the values for the features in the `features` parameter (the order matters)\n",
    "- `features`: the names of the features being passed\n",
    "- `breed`: the breed that will be assumed for the `X` observation\n",
    "- `params_dict`: the dictionary containing the estimated parameters from the training split\n",
    "\n",
    "The function should return the probability of the values of `X` given the selected `breed`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28f50bb9",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def prob_of_X_given_C(X, features, breed, params_dict):\n",
    "    \"\"\"\n",
    "    Calculate the conditional probability of X given a specific breed, using the given features and parameters.\n",
    "\n",
    "    Args:\n",
    "        X (list): List of feature values for which the probability needs to be calculated.\n",
    "        features (list): List of feature names corresponding to the feature values in X.\n",
    "        breed (str): The breed for which the probability is calculated.\n",
    "        params_dict (dict): Dictionary containing the parameters for different breeds and features.\n",
    "\n",
    "    Returns:\n",
    "        float: The conditional probability of X given the specified breed.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(X) != len(features):\n",
    "        print(\"X and list of features should have the same length\")\n",
    "        return 0\n",
    "    \n",
    "    probability = 1.0\n",
    "    \n",
    "    for x, feature in zip(X, features):\n",
    "        \n",
    "        # Get the relevant parameters from params_dict \n",
    "        params = params_dict[breed][feature]\n",
    "\n",
    "        match feature:\n",
    "            # You can add add as many case statements as you see fit\n",
    "            case \"height\" | \"weight\": \n",
    "                # Compute the relevant pdf given the distribution and the estimated parameters\n",
    "                probability_f = pdf_gaussian(x, params.mu, params.sigma)\n",
    "                \n",
    "            case \"bark_days\": \n",
    "                # Compute the relevant pdf given the distribution and the estimated parameters\n",
    "                probability_f = pdf_binomial(x, params.n, params.p)\n",
    "\n",
    "            case \"ear_head_ratio\": \n",
    "                # Compute the relevant pdf given the distribution and the estimated parameters\n",
    "                probability_f = pdf_uniform(x, params.a, params.b)\n",
    "        \n",
    "        # Multiply by probability of current feature\n",
    "        probability *= probability_f\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c00030d",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example dog has breed 1 and features: height = 28.63, weight = 21.56, bark_days = 13.00, ear_head_ratio = 0.27\n",
      "\n",
      "Probability of these features if dog is classified as breed 0: 7.065771582111452e-11\n",
      "Probability of these features if dog is classified as breed 1: 0.003942085319420164\n",
      "Probability of these features if dog is classified as breed 2: 5.770264459662167e-08\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "example_dog = df_test[FEATURES].loc[0]\n",
    "example_breed = df_test[[\"breed\"]].loc[0][\"breed\"]\n",
    "print(f\"Example dog has breed {example_breed} and features: height = {example_dog['height']:.2f}, weight = {example_dog['weight']:.2f}, bark_days = {example_dog['bark_days']:.2f}, ear_head_ratio = {example_dog['ear_head_ratio']:.2f}\\n\")\n",
    "\n",
    "print(f\"Probability of these features if dog is classified as breed 0: {prob_of_X_given_C([*example_dog], FEATURES, 0, train_params)}\")\n",
    "print(f\"Probability of these features if dog is classified as breed 1: {prob_of_X_given_C([*example_dog], FEATURES, 1, train_params)}\")\n",
    "print(f\"Probability of these features if dog is classified as breed 2: {prob_of_X_given_C([*example_dog], FEATURES, 2, train_params)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e6a339",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Example dog has breed 1 and features: height = 28.63, weight = 21.56, bark_days = 13.00, ear_head_ratio = 0.27\n",
    "\n",
    "Probability of these features if dog is classified as breed 0: 6.989632718589114e-11\n",
    "Probability of these features if dog is classified as breed 1: 0.0038267778327024894\n",
    "Probability of these features if dog is classified as breed 2: 7.959172138800559e-08\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9771c79",
   "metadata": {},
   "source": [
    "## Exercise 7: Predict the Breed\n",
    "\n",
    "If all classes were perfectly balanced, the previous function could be used to compute the maximum posterior. However, this is NOT the case, and you still need to multiply every probability $\\mathbf P(x \\mid C_{i})$ by the probability of belonging to each class $\\mathbf P(C_{i})$. After all, the expression that you need to maximize in order to get a prediction is $\\mathbf P(x \\mid C_{i})\\mathbf P(C_{i})$. You can accomplish this by multiplying the result of `prob_of_X_given_C` by the corresponding proportion found in the `probs_dict` dictionary.\n",
    "\n",
    "Complete the `predict_breed` function below. This function receives the following parameters:\n",
    "- `X`: a list containing the values for the features of each feature in the `features` parameter (the order matters).\n",
    "- `features`: the name of the features being passed.\n",
    "- `params_dict`: the dictionary containing the estimated parameters from the training split.\n",
    "- `probs_dict`: the dictionary containing the proportion of each class from the training split.\n",
    "\n",
    "The function should return the breed with the highest maximum posterior:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "704ec4ac",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def predict_breed(X, features, params_dict, probs_dict):\n",
    "    \"\"\"\n",
    "    Predicts the breed based on the input and features.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): The input data for prediction.\n",
    "        features (array-like): The features used for prediction.\n",
    "        params_dict (dict): A dictionary containing parameters for different breeds.\n",
    "        probs_dict (dict): A dictionary containing probabilities for different breeds.\n",
    "\n",
    "    Returns:\n",
    "        int: The predicted breed index.\n",
    "    \"\"\"\n",
    "    \n",
    "    posterior_breed_0 = prob_of_X_given_C(X, features, 0, params_dict)*probs_dict[0] # @REPLACE posterior_breed_0 = prob_of_X_given_C(None, None, None, None)*probs_dict[None]\n",
    "    posterior_breed_1 = prob_of_X_given_C(X, features, 1, params_dict)*probs_dict[1] # @REPLACE posterior_breed_1 = prob_of_X_given_C(None, None, None, None)*probs_dict[None]\n",
    "    posterior_breed_2 = prob_of_X_given_C(X, features, 2, params_dict)*probs_dict[2] # @REPLACE posterior_breed_2 = prob_of_X_given_C(None, None, None, None)*probs_dict[None]\n",
    "    \n",
    "    # Save the breed with the maximum posterior\n",
    "    # Hint: You can create a numpy array with the posteriors and then use np.argmax\n",
    "    prediction = np.argmax(np.array([posterior_breed_0, posterior_breed_1, posterior_breed_2]))\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7315ee67",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example dog has breed 1 and Naive Bayes classified it as 1\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "example_pred = predict_breed([*example_dog], FEATURES, train_params, train_class_probs)\n",
    "print(f\"Example dog has breed {example_breed} and Naive Bayes classified it as {example_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4af42",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Example dog has breed 1 and Naive Bayes classified it as 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6563437b",
   "metadata": {},
   "source": [
    "The classifier worked for this particular example, but how will it perform when considering the whole testing split?\n",
    "\n",
    "Run the following cell to find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ca023fd",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for the test split: 1.00\n"
     ]
    }
   ],
   "source": [
    "preds = df_test.apply(lambda x: predict_breed([*x[FEATURES]], FEATURES, train_params, train_class_probs), axis=1)\n",
    "test_acc = accuracy_score(df_test[\"breed\"], preds)\n",
    "print(f\"Accuracy score for the test split: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8fde0",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Accuracy score for the test split: 1.00\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07594b8",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier achieved an accuracy of 100% in the testing data. Nice job! \n",
    "\n",
    "You might think that something is wrong when reaching such a high accuracy but in this case it makes sense because the data is generated and you know the true distributions for each feature, real-life data won't have this nice behavior!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9cb739",
   "metadata": {},
   "source": [
    "# Section 3 - Spam Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3c87f",
   "metadata": {},
   "source": [
    "In this final section you will once again implement and train a Naive Bayes classifier. The idea is to build a classifier that is able to detect spam from ham (aka not spam) emails. This time your implementation should take into account two major differences:\n",
    "\n",
    "- The data is a real life dataset that includes over 5500 emails with their corresponding labels\n",
    "- The features are categorical\n",
    "\n",
    "Begin by loading the dataset and doing some pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6aa97724",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "      <td>[task, are, suqgestions, much, -, company, rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "      <td>[plain, attire, deoxyribonucleic, inflexible, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[in, 3, new, fixed, advantage, 72, pittman, -,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "      <td>[), fax, /, canyon, pdf, rd, additional, !, -,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[yet, are, do, death, be, software, !, grow, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam  \\\n",
       "0  Subject: naturally irresistible your corporate...     1   \n",
       "1  Subject: the stock trading gunslinger  fanny i...     1   \n",
       "2  Subject: unbelievable new homes made easy  im ...     1   \n",
       "3  Subject: 4 color printing special  request add...     1   \n",
       "4  Subject: do not have money , get software cds ...     1   \n",
       "\n",
       "                                               words  \n",
       "0  [task, are, suqgestions, much, -, company, rea...  \n",
       "1  [plain, attire, deoxyribonucleic, inflexible, ...  \n",
       "2  [in, 3, new, fixed, advantage, 72, pittman, -,...  \n",
       "3  [), fax, /, canyon, pdf, rd, additional, !, -,...  \n",
       "4  [yet, are, do, death, be, software, !, grow, c...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "emails = pd.read_csv('emails.csv')\n",
    "\n",
    "# Helper function that converts text to lowercase and splits words into a list\n",
    "def process_email(text):\n",
    "    \"\"\"\n",
    "    Processes the given email text by converting it to lowercase, splitting it into words,\n",
    "    and returning a list of unique words.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The email text to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of unique words extracted from the email text.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    return list(set(text.split()))\n",
    "\n",
    "# Create an extra column with the text converted to a lower-cased list of words\n",
    "emails['words'] = emails['text'].apply(process_email)\n",
    "\n",
    "# Show the first 5 rows\n",
    "emails.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bf5d9e",
   "metadata": {},
   "source": [
    "## One last Naive Bayes reminder\n",
    "\n",
    "Remember that supposing **class-conditional independence**, for a $\\boldsymbol x = (x_1, \\ldots x_n)$ in $\\boldsymbol X$:\n",
    "\n",
    "$$\\mathbf P(\\boldsymbol x \\mid C_{i}) = \\mathbf P(x_1 \\mid C_i) \\cdot \\mathbf P(x_2 \\mid C_i) \\cdot \\ldots \\cdot \\mathbf P(x_n \\mid C_i) = \\prod_{k = 1}^{n} \\mathbf P(x_k \\mid C_i).$$\n",
    "\n",
    "The probabilities $\\mathbf P(x_k\\mid C_i)$ can be estimated from the training tuples. The computation of $\\mathbf P(x_k \\mid C_i)$ depends on whether $x_k$ is categorical or not.\n",
    "\n",
    "If $x_k$ is categorical, then $\\mathbf P(x_k \\mid C_i)$ is the number of samples in $X$ that has attribute $x_k$ divided by the number of samples in class $C_i$. \n",
    "\n",
    "With this in mind, you need to know the number of times that each word appears in both spam and ham emails, as well as the number of samples for class. Your next two exercises will be about computing these values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb834c",
   "metadata": {},
   "source": [
    "## Exercise 8: Frequency of a word in each class\n",
    "\n",
    "To compute the frequency of each word in the dataset you need to define the `word_freq_per_class` below. This function receives the email dataframe as input and should return a dictionary that has the words in the emails as keys and another dictionary that keeps track of how many times that word appeared in `spam` and `hams` emails as values. This dictionary should look like this:\n",
    "\n",
    "```python\n",
    "{'website': {'spam': 204, 'ham': 135},\n",
    " 'collaboration': {'spam': 28, 'ham': 34},\n",
    " 'logo': {'spam': 97, 'ham': 13},\n",
    " ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "698c0c15",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def word_freq_per_class(df):\n",
    "    \"\"\"\n",
    "    Calculates the frequency of words in each class (spam and ham) based on a given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input dataframe containing email data, \n",
    "        with a column named 'words' representing the words in each email.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the frequency of words in each class. \n",
    "        The keys of the dictionary are words, and the values are nested dictionaries with keys \n",
    "        'spam' and 'ham' representing the frequency of the word in spam and ham emails, respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_freq_dict = {}\n",
    "    \n",
    "    # Hint: You can use the iterrows() method to iterate over the rows of a dataframe.\n",
    "    # This method yields an index and the data in the row so you can ignore the first returned value. \n",
    "    for _, email in df.iterrows(): # @REPLACE for _, None in None:\n",
    "        # Iterate over the words in each email\n",
    "        for word in email['words']: # @REPLACE for None in None:\n",
    "            # Check if word doesn't exist within the dictionary\n",
    "            if word not in word_freq_dict: # @REPLACE if None not in None:\n",
    "                # If word doesn't exist, initialize the count at 0\n",
    "                word_freq_dict[word] = {'spam': 0, 'ham': 0} # @REPLACE word_freq_dict[None] = {None: None, None: None}\n",
    "            \n",
    "            # Check if the email was spam\n",
    "            match email['spam']: # @REPLACE match None[None]:\n",
    "                case 0: # @KEEP\n",
    "                    # If ham then add 1 to the count of ham\n",
    "                    word_freq_dict[word]['ham'] += 1 # @REPLACE word_freq_dict[None][None] += None\n",
    "                case 1: # @KEEP\n",
    "                    # If spam then add 1 to the count of spam\n",
    "                    word_freq_dict[word]['spam'] += 1 # @REPLACE word_freq_dict[None][None] += None\n",
    "\n",
    "    return word_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce2d6cbb",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency in both classes for word 'lottery': {'spam': 8, 'ham': 0}\n",
      "\n",
      "Frequency in both classes for word 'sale': {'spam': 38, 'ham': 41}\n",
      "\n",
      "Word 'asdfg' not in corpus\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "word_freq = word_freq_per_class(emails)\n",
    "print(f\"Frequency in both classes for word 'lottery': {word_freq['lottery']}\\n\")\n",
    "print(f\"Frequency in both classes for word 'sale': {word_freq['sale']}\\n\")\n",
    "\n",
    "try:\n",
    "    word_freq['asdfg']\n",
    "except KeyError:\n",
    "    print(\"Word 'asdfg' not in corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab1fcd",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Frequency in both classes for word 'lottery': {'spam': 8, 'ham': 0}\n",
    "\n",
    "Frequency in both classes for word 'sale': {'spam': 38, 'ham': 41}\n",
    "\n",
    "Word 'asdfg' not in corpus\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe8acc",
   "metadata": {},
   "source": [
    "## Exercise 9: Frequency of classes\n",
    "\n",
    "To compute the frequency of each class in the dataset you need to define the `class_frequencies` below. This function receives the email dataframe as input and should return a dictionary that has returns the number of spam and ham emails. This dictionary should look like this:\n",
    "\n",
    "```python\n",
    "{'spam': 1000, 'ham': 1000}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5f03a74",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def class_frequencies(df):\n",
    "    \"\"\"\n",
    "    Calculate the frequencies of classes in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame containing a column 'spam' indicating class labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the frequencies of the classes.\n",
    "            The keys are 'spam' and 'ham', representing the class labels.\n",
    "            The values are the corresponding frequencies in the DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    class_freq_dict = { # @KEEP\n",
    "        \"spam\": len(df[df['spam']==1]), # @REPLACE \"spam\": None,\n",
    "        \"ham\": len(df[df['spam']==0]) # @REPLACE \"ham\": None\n",
    "    } # @KEEP\n",
    "    \n",
    "    return class_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20eebe98",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small dataset:\n",
      "\n",
      "The frequencies for each class are {'spam': 10, 'ham': 0}\n",
      "\n",
      "The proportion of spam in the dataset is: 100.00%\n",
      "\n",
      "The proportion of ham in the dataset is: 0.00%\n",
      "\n",
      "\n",
      "Full dataset:\n",
      "\n",
      "The frequencies for each class are {'spam': 1368, 'ham': 4360}\n",
      "\n",
      "The proportion of spam in the dataset is: 23.88%\n",
      "\n",
      "The proportion of ham in the dataset is: 76.12%\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "class_freq = class_frequencies(emails[:10])\n",
    "print(f\"Small dataset:\\n\\nThe frequencies for each class are {class_freq}\\n\")\n",
    "print(f\"The proportion of spam in the dataset is: {100*class_freq['spam']/len(emails[:10]):.2f}%\\n\")\n",
    "print(f\"The proportion of ham in the dataset is: {100*class_freq['ham']/len(emails[:10]):.2f}%\\n\")\n",
    "\n",
    "class_freq = class_frequencies(emails)\n",
    "print(f\"\\nFull dataset:\\n\\nThe frequencies for each class are {class_freq}\\n\")\n",
    "print(f\"The proportion of spam in the dataset is: {100*class_freq['spam']/len(emails):.2f}%\\n\")\n",
    "print(f\"The proportion of ham in the dataset is: {100*class_freq['ham']/len(emails):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d3d34",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Small dataset:\n",
    "\n",
    "The frequencies for each class are {'spam': 10, 'ham': 0}\n",
    "\n",
    "The proportion of spam in the dataset is: 100.00%\n",
    "\n",
    "The proportion of ham in the dataset is: 0.00%\n",
    "\n",
    "\n",
    "Full dataset:\n",
    "\n",
    "The frequencies for each class are {'spam': 1368, 'ham': 4360}\n",
    "\n",
    "The proportion of spam in the dataset is: 23.88%\n",
    "\n",
    "The proportion of ham in the dataset is: 76.12%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f1a94c",
   "metadata": {},
   "source": [
    "## Exercise 10: Naive Bayes for categorical features\n",
    "\n",
    "Now you have everything you need to build your Naive Bayes classifier. Complete the `naive_bayes_classifier` below. This function receives any text as a parameter and should return the probability of that text belonging to the `spam` class. Notice that the function also receives the two dictionaries that were created during the previous exercises, which means that this probability will depend on the dataset you used for training. With this in mind, if you submit a text containing words that are not in the training dataset the probability should be equal to the proportion of `spam`  in the emails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e05f5ff",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(text, word_freq=word_freq, class_freq=class_freq):\n",
    "    \"\"\"\n",
    "    Implements a naive Bayes classifier to determine the probability of an email being spam.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input email text to classify.\n",
    "        \n",
    "        word_freq (dict): A dictionary containing word frequencies in the training corpus. \n",
    "        The keys are words, and the values are dictionaries containing frequencies for 'spam' and 'ham' classes.\n",
    "\n",
    "        class_freq (dict): A dictionary containing class frequencies in the training corpus. \n",
    "        The keys are class labels ('spam' and 'ham'), and the values are the respective frequencies.\n",
    "\n",
    "    Returns:\n",
    "        float: The probability of the email being spam.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    words = set(text.split())\n",
    "    cumulative_product_spam = 1.0\n",
    "    cumulative_product_ham = 1.0\n",
    "    \n",
    "    # Iterate over the words in the email\n",
    "    for word in words: # @REPLACE for None in None:\n",
    "        # You should only include words that exist in the corpus in your calculations\n",
    "        if word in word_freq: # @REPLACE if None in None:\n",
    "            cumulative_product_spam *= word_freq[word]['spam']/class_freq['spam'] # @REPLACE cumulative_product_spam *= None/None\n",
    "            cumulative_product_ham *= word_freq[word]['ham']/class_freq['ham'] # @REPLACE cumulative_product_ham *= None/None\n",
    "    \n",
    "     # Calculate the likelihood of the words appearing in the email given that it is spam\n",
    "    likelihood_word_given_spam = cumulative_product_spam * class_freq['spam'] # @REPLACE EQUALS None * None\n",
    "    \n",
    "    # Calculate the likelihood of the words appearing in the email given that it is ham\n",
    "    likelihood_word_given_ham = cumulative_product_ham * class_freq['ham'] # @REPLACE EQUALS None * None\n",
    "    \n",
    "    # Calculate the posterior probability of the email being spam given that the words appear in the email (the probability of being a spam given the email content)\n",
    "    prob_spam = likelihood_word_given_spam / (likelihood_word_given_spam + likelihood_word_given_ham)  # @REPLACE EQUALS None / (None + None)\n",
    "    \n",
    "    return prob_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b53b92c4",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of spam for email 'enter the lottery to win three million dollars': 100.00%\n",
      "\n",
      "Probability of spam for email 'meet me at the lobby of the hotel at nine am': 0.00%\n",
      "\n",
      "Probability of spam for email '9898 asjfkjfdj': 23.88%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "\n",
    "msg = \"enter the lottery to win three million dollars\"\n",
    "print(f\"Probability of spam for email '{msg}': {100*naive_bayes_classifier(msg):.2f}%\\n\")\n",
    "\n",
    "msg = \"meet me at the lobby of the hotel at nine am\"\n",
    "print(f\"Probability of spam for email '{msg}': {100*naive_bayes_classifier(msg):.2f}%\\n\")\n",
    "\n",
    "msg = \"9898 asjfkjfdj\"\n",
    "print(f\"Probability of spam for email '{msg}': {100*naive_bayes_classifier(msg):.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a3df24",
   "metadata": {},
   "source": [
    "##### __Expected Output__\n",
    "\n",
    "```\n",
    "Probability of spam for email 'enter the lottery to win three million dollars': 100.00%\n",
    "\n",
    "Probability of spam for email 'meet me at the lobby of the hotel at nine am': 0.00%\n",
    "\n",
    "Probability of spam for email '9898 asjfkjfdj': 23.88%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bef59b",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this assignment!**\n",
    "\n",
    "During this assignment you tested your theoretical and practical skills by coding functions capable of generating random numbers for the probability distributions you saw in the lectures, as well as creating two implementations of the Naive Bayes algorithm.\n",
    "\n",
    "**Keep up the good work!**\n"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
